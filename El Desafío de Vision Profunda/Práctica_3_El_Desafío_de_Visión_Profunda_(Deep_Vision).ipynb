{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVuIMpDbvXPU",
        "outputId": "86117471-3ba9-4d05-eb85-e8a8ad6a79a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estructura de repo creada en: /content/Práctica 3: El Desafío de Visión Profunda (Deep Vision)_RellosoDaniel\n"
          ]
        }
      ],
      "source": [
        "\n",
        "APELLIDO = \"Relloso\"\n",
        "NOMBRE = \"Daniel\"\n",
        "REPO_NAME = f\"Práctica 3: El Desafío de Visión Profunda (Deep Vision)_{APELLIDO}{NOMBRE}\"\n",
        "\n",
        "import os, random, json, datetime, hashlib\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# --- Fijar semillas (Python / NumPy / TF) ---\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# --- Crear estructura mínima del repo (en Colab: carpeta local) ---\n",
        "base_dir = Path(REPO_NAME)\n",
        "notebooks_dir = base_dir / \"notebooks\"\n",
        "results_dir = base_dir / \"results\"\n",
        "figuras_dir = base_dir / \"figuras\"\n",
        "outputs_dir = base_dir / \"outputs\"\n",
        "env_dir = base_dir / \"env\"\n",
        "\n",
        "for d in [base_dir, notebooks_dir, results_dir, figuras_dir, outputs_dir, env_dir]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# README y .gitignore mínimos (si no existen)\n",
        "readme_path = base_dir / \"README.md\"\n",
        "if not readme_path.exists():\n",
        "    readme_path.write_text(\"# Proyecto CIFAR-10 CNN\\n\\nPráctica de IA sobre CIFAR-10.\\n\")\n",
        "\n",
        "gitignore_path = base_dir / \".gitignore\"\n",
        "if not gitignore_path.exists():\n",
        "    gitignore_path.write_text(\"\"\"__pycache__/\n",
        ".ipynb_checkpoints/\n",
        "env/\n",
        "outputs/\n",
        "results/\n",
        "\"\"\")\n",
        "\n",
        "print(\"Estructura de repo creada en:\", base_dir.resolve())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "import pkg_resources\n",
        "\n",
        "env_md_path = env_dir / \"ENVIRONMENT.md\"\n",
        "req_path = env_dir / \"requirements.txt\"\n",
        "\n",
        "# Info de versiones básicas\n",
        "python_version = platform.python_version()\n",
        "tf_version = tf.__version__\n",
        "\n",
        "# Info GPU (si existe)\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "gpu_info = \"GPU disponible: \" + (gpus[0].name if gpus else \"No\")\n",
        "\n",
        "env_text = f\"\"\"# Entorno de ejecución\n",
        "\n",
        "- Python: {python_version}\n",
        "- TensorFlow: {tf_version}\n",
        "- {gpu_info}\n",
        "\"\"\"\n",
        "\n",
        "env_md_path.write_text(env_text)\n",
        "\n",
        "# Congelar dependencias principales del entorno actual\n",
        "installed_packages = sorted(\n",
        "    [f\"{d.project_name}=={d.version}\" for d in pkg_resources.working_set]\n",
        ")\n",
        "\n",
        "req_path.write_text(\"\\n\".join(installed_packages))\n",
        "\n",
        "print(\"ENVIRONMENT.md y requirements.txt creados en env/\")\n"
      ],
      "metadata": {
        "id": "kQn4zunyyK2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "(x_train_full, y_train_full), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print(\"x_train_full shape:\", x_train_full.shape)\n",
        "print(\"y_train_full shape:\", y_train_full.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "rHYjNzn2x09c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# y_train_full está en forma (50000,1) -> lo aplano a vector 1D\n",
        "y_flat = y_train_full.ravel()\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(\n",
        "    x_train_full,\n",
        "    y_flat,\n",
        "    test_size=0.2,\n",
        "    random_state=SEED,\n",
        "    stratify=y_flat\n",
        ")\n",
        "\n",
        "print(\"x_train:\", x_train.shape)\n",
        "print(\"y_train:\", y_train.shape)\n",
        "print(\"x_valid:\", x_valid.shape)\n",
        "print(\"y_valid:\", y_valid.shape)\n"
      ],
      "metadata": {
        "id": "yGM6hn5RyU3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Normalizar a [0,1]\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_valid = x_valid.astype(\"float32\") / 255.0\n",
        "x_test  = x_test.astype(\"float32\")  / 255.0\n",
        "\n",
        "# One-hot\n",
        "NUM_CLASSES = 10\n",
        "y_train_oh = to_categorical(y_train, NUM_CLASSES)\n",
        "y_valid_oh = to_categorical(y_valid, NUM_CLASSES)\n",
        "y_test_oh  = to_categorical(y_test, NUM_CLASSES)\n",
        "\n",
        "print(\"x_train min/max:\", x_train.min(), x_train.max())\n",
        "print(\"y_train_oh shape:\", y_train_oh.shape)\n",
        "print(\"y_valid_oh shape:\", y_valid_oh.shape)\n",
        "print(\"y_test_oh shape:\", y_test_oh.shape)\n"
      ],
      "metadata": {
        "id": "bs_JMoUYyW41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class_names = [\n",
        "    \"airplane\", \"automobile\", \"bird\", \"cat\",\n",
        "    \"deer\", \"dog\", \"frog\", \"horse\",\n",
        "    \"ship\", \"truck\"\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "for i in range(16):\n",
        "    idx = i  # puedes mezclar si quieres: np.random.randint(0, x_train.shape[0])\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    plt.imshow(x_train[idx])\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(class_names[y_train[idx]])\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KOtrFU8cyYdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "# --- R4: data_meta.json con formas, fecha/hora y hash de primeras 1024 imágenes ---\n",
        "\n",
        "def compute_data_hash(x, n_samples=1024):\n",
        "    subset = x[:n_samples]\n",
        "    # Convertimos a bytes (tras normalización)\n",
        "    data_bytes = subset.tobytes()\n",
        "    h = hashlib.sha256(data_bytes).hexdigest()\n",
        "    return h\n",
        "\n",
        "data_meta = {\n",
        "    \"shapes\": {\n",
        "        \"x_train\": list(x_train.shape),\n",
        "        \"x_valid\": list(x_valid.shape),\n",
        "        \"x_test\":  list(x_test.shape),\n",
        "        \"y_train_oh\": list(y_train_oh.shape),\n",
        "        \"y_valid_oh\": list(y_valid_oh.shape),\n",
        "        \"y_test_oh\":  list(y_test_oh.shape),\n",
        "    },\n",
        "    \"datetime\": datetime.datetime.now().isoformat(),\n",
        "    \"hash\": compute_data_hash(x_train, n_samples=1024)\n",
        "}\n",
        "\n",
        "data_meta_path = results_dir / \"data_meta.json\"\n",
        "with data_meta_path.open(\"w\") as f:\n",
        "    json.dump(data_meta, f, indent=4)\n",
        "\n",
        "print(\"Guardado:\", data_meta_path)\n",
        "\n",
        "# --- R3: params.yaml con config inicial del modelo/entrenamiento ---\n",
        "\n",
        "params = {\n",
        "    \"model\": {\n",
        "        \"blocks\": 2,\n",
        "        \"filters\": [32, 64],\n",
        "        \"kernel_size\": 3,\n",
        "        \"dense_units\": 128,\n",
        "        \"dropout\": 0.5\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"lr\": 1e-3,\n",
        "        \"batch_size\": 64,\n",
        "        \"epochs\": 30,\n",
        "        \"augment\": False,\n",
        "        \"seed\": SEED\n",
        "    }\n",
        "}\n",
        "\n",
        "params_path = results_dir / \"params.yaml\"\n",
        "with params_path.open(\"w\") as f:\n",
        "    yaml.dump(params, f, sort_keys=False)\n",
        "\n",
        "print(\"Guardado:\", params_path)\n"
      ],
      "metadata": {
        "id": "PWFvaA46ybJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smoke_test_model():\n",
        "    try:\n",
        "        from tensorflow.keras import layers, models\n",
        "\n",
        "        model = models.Sequential([\n",
        "            layers.Input(shape=(32, 32, 3)),\n",
        "            layers.Conv2D(8, (3, 3), activation=\"relu\"),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(10, activation=\"softmax\")\n",
        "        ])\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "            loss=\"categorical_crossentropy\",\n",
        "            metrics=[\"accuracy\"]\n",
        "        )\n",
        "        print(\"Smoke test OK: modelo construido y compilado.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"Smoke test FAILED:\", e)\n",
        "        return False\n",
        "\n",
        "smoke_test_model()\n"
      ],
      "metadata": {
        "id": "xeRrrqbDyd5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"x_train:\", x_train.shape)\n",
        "print(\"x_valid:\", x_valid.shape)\n",
        "print(\"x_test:\", x_test.shape)\n",
        "\n",
        "print(\"y_train_oh:\", y_train_oh.shape)\n",
        "print(\"y_valid_oh:\", y_valid_oh.shape)\n",
        "print(\"y_test_oh:\", y_test_oh.shape)\n",
        "\n",
        "print(\"data_meta.json existe:\", data_meta_path.exists())\n",
        "print(\"params.yaml existe:\", params_path.exists())\n"
      ],
      "metadata": {
        "id": "a4ZKFaiCyfZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Por qué normalizar /255?\n",
        "Porque los píxeles vienen en el rango [0,255] y son valores relativamente grandes y desbalanceados para las capas iniciales. Al dividir entre 255 llevamos todo a [0,1], lo que hace que las activaciones y los gradientes se mantengan en rangos más estables, permite usar tasas de aprendizaje más razonables y facilita que el optimizador encuentre una buena solución.\n",
        "\n",
        "¿Por qué estratificar aquí y no al final?\n",
        "Porque quiero que desde el principio el split train/valid mantenga la misma proporción de clases que el conjunto original. Si estratifico sobre y_train_full antes de cualquier otro procesamiento, me aseguro de que tanto x_train como x_valid sean representativos del problema real. Si hiciera el split al final, después de alguna transformación rara (por ejemplo, filtrar clases o reequilibrar), podría romper esa representatividad o acabar con un conjunto de validación pobremente balanceado, lo que sesgaría la evaluación del modelo."
      ],
      "metadata": {
        "id": "Lb2GuVDIyh0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import yaml\n",
        "\n",
        "# === Ajusta estos nombres igual que en el PROMPT 1 ===\n",
        "APELLIDO = \"Relloso\"   # cámbialo\n",
        "NOMBRE = \"Daniel\"       # cámbialo\n",
        "REPO_NAME = f\"IA_P3_CIFAR10_{APELLIDO}{NOMBRE}\"\n",
        "\n",
        "base_dir    = Path(REPO_NAME)\n",
        "results_dir = base_dir / \"results\"\n",
        "figuras_dir = base_dir / \"figuras\"\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "figuras_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# === Definición del MLP baseline ===\n",
        "mlp_model = models.Sequential([\n",
        "    layers.Input(shape=(32, 32, 3)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation=\"relu\"),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "mlp_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Mostrar y guardar summary (para trazabilidad)\n",
        "summary_path = results_dir / \"mlp_summary.txt\"\n",
        "with open(summary_path, \"w\") as f:\n",
        "    mlp_model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
        "\n",
        "mlp_model.summary()\n",
        "print(\"Resumen guardado en:\", summary_path)\n",
        "\n",
        "# === Entrenamiento 10 épocas, batch 64, con validación ===\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "mlp_history = mlp_model.fit(\n",
        "    x_train, y_train_oh,\n",
        "    validation_data=(x_valid, y_valid_oh),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "16T3UHIByio8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Evaluación en validación y test ===\n",
        "val_loss, val_acc = mlp_model.evaluate(x_valid, y_valid_oh, verbose=0)\n",
        "test_loss, test_acc = mlp_model.evaluate(x_test, y_test_oh, verbose=0)\n",
        "print(f\"Validación - loss: {val_loss:.4f}, acc: {val_acc:.4f}\")\n",
        "print(f\"Test       - loss: {test_loss:.4f}, acc: {test_acc:.4f}\")\n",
        "\n",
        "# === Curvas de entrenamiento ===\n",
        "history_dict = mlp_history.history\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history_dict[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(history_dict[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss MLP\")\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history_dict[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(history_dict[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy MLP\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Nombre de fichero con fecha + commit corto\n",
        "from datetime import datetime\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Cambia este string por el hash corto real de tu commit\n",
        "commit_short = \"ab12cd\"\n",
        "\n",
        "fig_name = f\"{timestamp}_{commit_short}_mlp_curvas.png\"\n",
        "fig_path = figuras_dir / fig_name\n",
        "plt.savefig(fig_path, dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Figura guardada en:\", fig_path)\n"
      ],
      "metadata": {
        "id": "2ci516uJyy20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Exportar history a CSV ===\n",
        "history_df = pd.DataFrame(history_dict)\n",
        "history_csv_path = results_dir / \"history.csv\"\n",
        "history_df.to_csv(history_csv_path, index=False)\n",
        "print(\"History guardado en:\", history_csv_path)\n"
      ],
      "metadata": {
        "id": "B0oG5CUUy1Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_path = results_dir / \"params.yaml\"\n",
        "\n",
        "if params_path.exists():\n",
        "    with open(params_path, \"r\") as f:\n",
        "        params = yaml.safe_load(f)\n",
        "else:\n",
        "    params = {}\n",
        "\n",
        "# Añadimos o actualizamos la sección del MLP baseline\n",
        "params[\"mlp_baseline\"] = {\n",
        "    \"architecture\": \"Input(32,32,3) -> Flatten -> Dense(256,relu) -> Dropout(0.5) -> Dense(10,softmax)\",\n",
        "    \"hidden_units\": 256,\n",
        "    \"dropout\": 0.5,\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"seed\": 42\n",
        "}\n",
        "\n",
        "with open(params_path, \"w\") as f:\n",
        "    yaml.dump(params, f, sort_keys=False)\n",
        "\n",
        "print(\"params.yaml actualizado en:\", params_path)\n"
      ],
      "metadata": {
        "id": "RzCZNzNPy2bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_path = results_dir / \"metrics.json\"\n",
        "\n",
        "current_metrics = {\n",
        "    \"model\": \"mlp_baseline\",\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"val_loss\": float(val_loss),\n",
        "    \"val_acc\": float(val_acc),\n",
        "    \"test_loss\": float(test_loss),\n",
        "    \"test_acc\": float(test_acc),\n",
        "    \"commit\": commit_short\n",
        "}\n",
        "\n",
        "if metrics_path.exists():\n",
        "    with open(metrics_path, \"r\") as f:\n",
        "        try:\n",
        "            metrics_list = json.load(f)\n",
        "            if not isinstance(metrics_list, list):\n",
        "                metrics_list = [metrics_list]\n",
        "        except json.JSONDecodeError:\n",
        "            metrics_list = []\n",
        "else:\n",
        "    metrics_list = []\n",
        "\n",
        "metrics_list.append(current_metrics)\n",
        "\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    json.dump(metrics_list, f, indent=4)\n",
        "\n",
        "print(\"metrics.json actualizado en:\", metrics_path)\n"
      ],
      "metadata": {
        "id": "dsthKldry5He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Resumen guardado:\", summary_path.exists())\n",
        "print(\"History CSV:\", history_csv_path.exists())\n",
        "print(\"Metrics JSON:\", metrics_path.exists())\n",
        "print(\"Figura curvas:\", fig_path.exists())\n"
      ],
      "metadata": {
        "id": "DtJhdHdhy6rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Hay overfitting?\n",
        "En las curvas se ve que la loss de entrenamiento sigue bajando y la accuracy de train sube más que la de validación. A partir de unas pocas épocas, la loss de validación se estabiliza o incluso empeora ligeramente mientras el modelo sigue mejorando en train. Esto indica un cierto overfitting: el MLP empieza a memorizar patrones específicos del conjunto de entrenamiento y deja de mejorar en datos nuevos.\n",
        "\n",
        "¿Por qué el aplanado limita la generalización?\n",
        "Al aplanar la imagen 32×32×3 a un vector de 3072 valores, el modelo pierde toda la estructura espacial: ya no sabe qué píxeles son vecinos ni puede explotar patrones locales (bordes, texturas, partes del objeto). Cada neurona densa ve la imagen como una lista de números sin geometría. Eso obliga al MLP a “aprender desde cero” relaciones que una CNN incorpora de forma natural mediante convoluciones y peso compartido. El resultado es un modelo con muchos parámetros, más sensible al ruido de fondo y con peor capacidad de generalizar en imágenes que una CNN diseñada para explotar la estructura espacial."
      ],
      "metadata": {
        "id": "SiCZi_eGy8q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tensorflow.keras import layers, models\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import json, yaml\n",
        "from datetime import datetime\n",
        "\n",
        "# Mismos nombres que antes\n",
        "APELLIDO = \"Relloso\"   # cámbialo\n",
        "NOMBRE = \"DAniel\"       # cámbialo\n",
        "REPO_NAME = f\"IA_P3_CIFAR10_{APELLIDO}{NOMBRE}\"\n",
        "\n",
        "base_dir    = Path(REPO_NAME)\n",
        "results_dir = base_dir / \"results\"\n",
        "figuras_dir = base_dir / \"figuras\"\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "figuras_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# === Definir CNN de 2 bloques ===\n",
        "cnn_model = models.Sequential([\n",
        "    layers.Input(shape=(32, 32, 3)),\n",
        "    layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation=\"relu\"),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "cnn_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Guardar summary para trazabilidad\n",
        "cnn_summary_path = results_dir / \"cnn_2blocks_summary.txt\"\n",
        "with open(cnn_summary_path, \"w\") as f:\n",
        "    cnn_model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
        "\n",
        "cnn_model.summary()\n",
        "print(\"Resumen CNN guardado en:\", cnn_summary_path)\n",
        "\n",
        "# === Entrenar 15 épocas, batch=64, con validación y temporizando ===\n",
        "EPOCHS_CNN = 15\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "start_time = time.time()\n",
        "cnn_history = cnn_model.fit(\n",
        "    x_train, y_train_oh,\n",
        "    validation_data=(x_valid, y_valid_oh),\n",
        "    epochs=EPOCHS_CNN,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=1\n",
        ")\n",
        "total_time = time.time() - start_time\n",
        "time_per_epoch_cnn = total_time / EPOCHS_CNN\n",
        "print(f\"Tiempo total entrenamiento CNN: {total_time:.2f} s\")\n",
        "print(f\"Tiempo medio por época CNN: {time_per_epoch_cnn:.2f} s\")\n"
      ],
      "metadata": {
        "id": "g0-GOBoky9NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluación en validación y test\n",
        "cnn_val_loss, cnn_val_acc = cnn_model.evaluate(x_valid, y_valid_oh, verbose=0)\n",
        "cnn_test_loss, cnn_test_acc = cnn_model.evaluate(x_test, y_test_oh, verbose=0)\n",
        "print(f\"Validación CNN - loss: {cnn_val_loss:.4f}, acc: {cnn_val_acc:.4f}\")\n",
        "print(f\"Test CNN       - loss: {cnn_test_loss:.4f}, acc: {cnn_test_acc:.4f}\")\n",
        "\n",
        "cnn_history_dict = cnn_history.history\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(cnn_history_dict[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(cnn_history_dict[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss CNN 2-block\")\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(cnn_history_dict[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(cnn_history_dict[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy CNN 2-block\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "commit_short_cnn = \"ef34gh\"  # cambia esto por el hash corto real\n",
        "\n",
        "fig_name_cnn = f\"{timestamp}_{commit_short_cnn}_cnn2blocks_curvas.png\"\n",
        "fig_path_cnn = figuras_dir / fig_name_cnn\n",
        "plt.savefig(fig_path_cnn, dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Figura CNN guardada en:\", fig_path_cnn)\n"
      ],
      "metadata": {
        "id": "lROV65ZuzeC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_history_df = pd.DataFrame(cnn_history_dict)\n",
        "cnn_history_csv_path = results_dir / \"history_cnn_2blocks.csv\"\n",
        "cnn_history_df.to_csv(cnn_history_csv_path, index=False)\n",
        "print(\"History CNN guardado en:\", cnn_history_csv_path)\n"
      ],
      "metadata": {
        "id": "eu8FBRWCzgPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_path = results_dir / \"metrics.json\"\n",
        "\n",
        "cnn_metrics = {\n",
        "    \"model\": \"cnn_2blocks\",\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"val_loss\": float(cnn_val_loss),\n",
        "    \"val_acc\": float(cnn_val_acc),\n",
        "    \"test_loss\": float(cnn_test_loss),\n",
        "    \"test_acc\": float(cnn_test_acc),\n",
        "    \"time_per_epoch\": float(time_per_epoch_cnn),\n",
        "    \"params\": int(cnn_model.count_params()),\n",
        "    \"commit\": commit_short_cnn\n",
        "}\n",
        "\n",
        "if metrics_path.exists():\n",
        "    with open(metrics_path, \"r\") as f:\n",
        "        try:\n",
        "            metrics_list = json.load(f)\n",
        "            if not isinstance(metrics_list, list):\n",
        "                metrics_list = [metrics_list]\n",
        "        except json.JSONDecodeError:\n",
        "            metrics_list = []\n",
        "else:\n",
        "    metrics_list = []\n",
        "\n",
        "metrics_list.append(cnn_metrics)\n",
        "\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    json.dump(metrics_list, f, indent=4)\n",
        "\n",
        "print(\"metrics.json actualizado con la CNN:\", metrics_path)\n"
      ],
      "metadata": {
        "id": "kmZfTLrVzhsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_path = results_dir / \"params.yaml\"\n",
        "\n",
        "if params_path.exists():\n",
        "    with open(params_path, \"r\") as f:\n",
        "        params = yaml.safe_load(f)\n",
        "else:\n",
        "    params = {}\n",
        "\n",
        "params[\"cnn_2blocks\"] = {\n",
        "    \"blocks\": 2,\n",
        "    \"filters\": [32, 64],\n",
        "    \"kernel_size\": 3,\n",
        "    \"dense_units\": 128,\n",
        "    \"dropout\": 0.5,\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"epochs\": EPOCHS_CNN,\n",
        "    \"seed\": 42\n",
        "}\n",
        "\n",
        "with open(params_path, \"w\") as f:\n",
        "    yaml.dump(params, f, sort_keys=False)\n",
        "\n",
        "print(\"params.yaml actualizado con la CNN en:\", params_path)\n"
      ],
      "metadata": {
        "id": "gQq4wwVszi8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o reutiliza estas variables del MLP:\n",
        "# - mlp_model (del prompt 2)\n",
        "# - val_acc, test_acc del MLP (puedes renombrarlos para evitar líos)\n",
        "# - time_per_epoch_mlp (mídelo con un timer, o pon el valor observado a mano)\n",
        "\n",
        "mlp_params = mlp_model.count_params()\n",
        "\n",
        "# Si guardaste las métricas del MLP en variables diferentes, ajústalo:\n",
        "val_acc_mlp  = float(val_acc)      # renombra si hace falta\n",
        "test_acc_mlp = float(test_acc)\n",
        "\n",
        "# Rellena esto con el valor real que observes\n",
        "time_per_epoch_mlp = 0.0  # sustituye por el tiempo medio real del MLP\n",
        "\n",
        "comparacion_df = pd.DataFrame([\n",
        "    {\n",
        "        \"modelo\": \"MLP baseline\",\n",
        "        \"params\": mlp_params,\n",
        "        \"time_per_epoch_s\": time_per_epoch_mlp,\n",
        "        \"val_acc\": val_acc_mlp,\n",
        "        \"test_acc\": test_acc_mlp\n",
        "    },\n",
        "    {\n",
        "        \"modelo\": \"CNN 2-blocks\",\n",
        "        \"params\": cnn_model.count_params(),\n",
        "        \"time_per_epoch_s\": time_per_epoch_cnn,\n",
        "        \"val_acc\": float(cnn_val_acc),\n",
        "        \"test_acc\": float(cnn_test_acc)\n",
        "    }\n",
        "])\n",
        "\n",
        "comparacion_path = results_dir / \"comparacion_mlp_vs_cnn.csv\"\n",
        "comparacion_df.to_csv(comparacion_path, index=False)\n",
        "\n",
        "print(comparacion_df)\n",
        "print(\"Tabla comparativa guardada en:\", comparacion_path)\n"
      ],
      "metadata": {
        "id": "Re8k-delzknJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Summary CNN guardado:\", cnn_summary_path.exists())\n",
        "print(\"History CNN:\", cnn_history_csv_path.exists())\n",
        "print(\"Metrics JSON:\", metrics_path.exists())\n",
        "print(\"Figura CNN:\", fig_path_cnn.exists())\n",
        "print(\"Tabla comparativa:\", comparacion_path.exists())\n"
      ],
      "metadata": {
        "id": "ppF80z5rzl0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Por qué la CNN puede rendir mejor con igual o menos parámetros?\n",
        "La CNN introduce un sesgo inductivo espacial: asume que las imágenes tienen estructura local y que los mismos patrones (bordes, texturas, partes de objetos) se repiten en distintas posiciones. Las capas convolucionales usan peso compartido: un mismo filtro se aplica en toda la imagen, lo que reduce mucho el número de parámetros frente a una capa densa que conecta cada píxel con cada neurona. Además, el pooling hace la representación más robusta a pequeñas traslaciones y ruido de fondo. Todo esto permite que la CNN “gaste” sus parámetros en capturar patrones visuales relevantes en vez de memorizar píxeles concretos, lo que suele dar mejor generalización en CIFAR-10 incluso con un número de parámetros similar o menor que el MLP baseline.\n"
      ],
      "metadata": {
        "id": "1oVU5_VyznvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import time, json, yaml\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Mismos nombres de repo que antes\n",
        "APELLIDO = \"Relloso\"   # cámbialo\n",
        "NOMBRE = \"Daniel\"       # cámbialo\n",
        "REPO_NAME = f\"IA_P3_CIFAR10_{APELLIDO}{NOMBRE}\"\n",
        "\n",
        "base_dir    = Path(REPO_NAME)\n",
        "results_dir = base_dir / \"results\"\n",
        "figuras_dir = base_dir / \"figuras\"\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "figuras_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# === Definir CNN 2 bloques con L2 ===\n",
        "weight_decay = 1e-4\n",
        "\n",
        "cnn_l2_model = models.Sequential([\n",
        "    layers.Input(shape=(32, 32, 3)),\n",
        "    layers.Conv2D(\n",
        "        32, (3, 3), activation=\"relu\", padding=\"same\",\n",
        "        kernel_regularizer=regularizers.l2(weight_decay)\n",
        "    ),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    layers.Conv2D(\n",
        "        64, (3, 3), activation=\"relu\", padding=\"same\",\n",
        "        kernel_regularizer=regularizers.l2(weight_decay)\n",
        "    ),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(\n",
        "        128, activation=\"relu\",\n",
        "        kernel_regularizer=regularizers.l2(weight_decay)\n",
        "    ),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "cnn_l2_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Guardar summary\n",
        "cnn_l2_summary_path = results_dir / \"cnn_2blocks_l2_es_summary.txt\"\n",
        "with open(cnn_l2_summary_path, \"w\") as f:\n",
        "    cnn_l2_model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
        "\n",
        "cnn_l2_model.summary()\n",
        "print(\"Resumen CNN L2+ES guardado en:\", cnn_l2_summary_path)\n"
      ],
      "metadata": {
        "id": "wxtBokN6zoGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS_L2 = 30\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "cnn_l2_history = cnn_l2_model.fit(\n",
        "    x_train, y_train_oh,\n",
        "    validation_data=(x_valid, y_valid_oh),\n",
        "    epochs=EPOCHS_L2,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "total_time_l2 = time.time() - start_time\n",
        "time_per_epoch_l2 = total_time_l2 / len(cnn_l2_history.history[\"loss\"])\n",
        "\n",
        "print(f\"Épocas realmente entrenadas: {len(cnn_l2_history.history['loss'])}\")\n",
        "print(f\"Tiempo total: {total_time_l2:.2f} s\")\n",
        "print(f\"Tiempo medio por época: {time_per_epoch_l2:.2f} s\")\n"
      ],
      "metadata": {
        "id": "qqYhkNtbz56B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_l2_val_loss, cnn_l2_val_acc = cnn_l2_model.evaluate(x_valid, y_valid_oh, verbose=0)\n",
        "cnn_l2_test_loss, cnn_l2_test_acc = cnn_l2_model.evaluate(x_test, y_test_oh, verbose=0)\n",
        "\n",
        "print(f\"Validación CNN L2+ES - loss: {cnn_l2_val_loss:.4f}, acc: {cnn_l2_val_acc:.4f}\")\n",
        "print(f\"Test CNN L2+ES       - loss: {cnn_l2_test_loss:.4f}, acc: {cnn_l2_test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "Dp9qLVK30D0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_l2_hist_dict = cnn_l2_history.history\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(cnn_l2_hist_dict[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(cnn_l2_hist_dict[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss CNN 2-blocks L2 + ES\")\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(cnn_l2_hist_dict[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(cnn_l2_hist_dict[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy CNN 2-blocks L2 + ES\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "commit_short_l2 = \"jk78lm\"  # pon aquí el hash corto real del commit\n",
        "\n",
        "fig_name_l2 = f\"{timestamp}_{commit_short_l2}_cnn2blocks_l2_es_curvas.png\"\n",
        "fig_path_l2 = figuras_dir / fig_name_l2\n",
        "plt.savefig(fig_path_l2, dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Figura CNN L2+ES guardada en:\", fig_path_l2)\n"
      ],
      "metadata": {
        "id": "yO87rLvU0FAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_l2_history_df = pd.DataFrame(cnn_l2_hist_dict)\n",
        "cnn_l2_history_csv_path = results_dir / \"history_cnn_2blocks_l2_es.csv\"\n",
        "cnn_l2_history_df.to_csv(cnn_l2_history_csv_path, index=False)\n",
        "print(\"History CNN L2+ES guardado en:\", cnn_l2_history_csv_path)\n"
      ],
      "metadata": {
        "id": "TzEekUdd0GUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_path = results_dir / \"metrics.json\"\n",
        "\n",
        "cnn_l2_metrics = {\n",
        "    \"model\": \"cnn_2blocks_l2_es\",\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"val_loss\": float(cnn_l2_val_loss),\n",
        "    \"val_acc\": float(cnn_l2_val_acc),\n",
        "    \"test_loss\": float(cnn_l2_test_loss),\n",
        "    \"test_acc\": float(cnn_l2_test_acc),\n",
        "    \"time_per_epoch\": float(time_per_epoch_l2),\n",
        "    \"params\": int(cnn_l2_model.count_params()),\n",
        "    \"l2\": float(weight_decay),\n",
        "    \"early_stopping\": {\n",
        "        \"monitor\": \"val_loss\",\n",
        "        \"patience\": 5,\n",
        "        \"restore_best_weights\": True,\n",
        "        \"epochs_trained\": len(cnn_l2_hist_dict[\"loss\"])\n",
        "    },\n",
        "    \"commit\": commit_short_l2\n",
        "}\n",
        "\n",
        "if metrics_path.exists():\n",
        "    with open(metrics_path, \"r\") as f:\n",
        "        try:\n",
        "            metrics_list = json.load(f)\n",
        "            if not isinstance(metrics_list, list):\n",
        "                metrics_list = [metrics_list]\n",
        "        except json.JSONDecodeError:\n",
        "            metrics_list = []\n",
        "else:\n",
        "    metrics_list = []\n",
        "\n",
        "metrics_list.append(cnn_l2_metrics)\n",
        "\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    json.dump(metrics_list, f, indent=4)\n",
        "\n",
        "print(\"metrics.json actualizado con CNN L2+ES:\", metrics_path)\n"
      ],
      "metadata": {
        "id": "HWxJIkwr0Hx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_path = results_dir / \"params.yaml\"\n",
        "\n",
        "if params_path.exists():\n",
        "    with open(params_path, \"r\") as f:\n",
        "        params = yaml.safe_load(f)\n",
        "else:\n",
        "    params = {}\n",
        "\n",
        "params[\"cnn_2blocks_l2_es\"] = {\n",
        "    \"blocks\": 2,\n",
        "    \"filters\": [32, 64],\n",
        "    \"kernel_size\": 3,\n",
        "    \"dense_units\": 128,\n",
        "    \"dropout\": 0.5,\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"max_epochs\": EPOCHS_L2,\n",
        "    \"seed\": 42,\n",
        "    \"l2\": weight_decay,\n",
        "    \"early_stopping\": {\n",
        "        \"monitor\": \"val_loss\",\n",
        "        \"patience\": 5,\n",
        "        \"restore_best_weights\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(params_path, \"w\") as f:\n",
        "    yaml.dump(params, f, sort_keys=False)\n",
        "\n",
        "print(\"params.yaml actualizado con CNN L2+ES en:\", params_path)\n"
      ],
      "metadata": {
        "id": "G6aSWvmm0JcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Resumen CNN L2+ES:\", cnn_l2_summary_path.exists())\n",
        "print(\"History L2+ES CSV:\", cnn_l2_history_csv_path.exists())\n",
        "print(\"Metrics JSON:\", metrics_path.exists())\n",
        "print(\"Figura L2+ES:\", fig_path_l2.exists())\n",
        "\n",
        "print(\"Épocas entrenadas (debería ser <= 30):\", len(cnn_l2_hist_dict[\"loss\"]))\n",
        "print(\"Mínima val_loss alcanzada:\", min(cnn_l2_hist_dict[\"val_loss\"]))\n"
      ],
      "metadata": {
        "id": "HPlYvJ280K-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Se redujo la brecha train/val?\n",
        "Con la regularización L2 y el EarlyStopping, la curva de loss de entrenamiento ya no se separa tanto de la loss de validación. El modelo deja de entrenarse cuando val_loss deja de mejorar, de modo que no sigue “bajando” solo en train mientras empeora en valid. En las curvas se ve que la diferencia entre train_loss y val_loss es más pequeña que en la CNN sin L2, lo que indica una brecha train/val reducida y menos sobreajuste.\n",
        "\n",
        "¿Subió la test accuracy?\n",
        "Aunque la accuracy de entrenamiento suele ser algo menor (el modelo está más “limitado” por la penalización L2 y la parada temprana), la test accuracy suele mantenerse o mejorar ligeramente respecto a la CNN sin regularización fuerte. Si la test_acc ha subido, es señal de que el modelo generaliza mejor. Si se mantiene muy parecida pero con una brecha train/val más pequeña, también es un buen resultado: se ha ganado estabilidad y robustez sin perder rendimiento en test. En cualquier caso, lo importante es que el modelo ya no se “dispara” en train mientras se degrada en valid, lo que refleja una generalización más sana."
      ],
      "metadata": {
        "id": "iXc4Mnts0MeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import time, json, yaml\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Mismos nombres que antes\n",
        "APELLIDO = \"Relloso\"   # cámbialo\n",
        "NOMBRE = \"Daniel\"       # cámbialo\n",
        "REPO_NAME = f\"IA_P3_CIFAR10_{APELLIDO}{NOMBRE}\"\n",
        "\n",
        "base_dir    = Path(REPO_NAME)\n",
        "results_dir = base_dir / \"results\"\n",
        "figuras_dir = base_dir / \"figuras\"\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "figuras_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Data Augmentation moderado ---\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.10),          # rotación ligera (±10% de vuelta)\n",
        "    layers.RandomZoom(0.10),              # zoom leve\n",
        "    layers.RandomTranslation(0.10, 0.10)  # traslación leve\n",
        "], name=\"data_augmentation\")\n",
        "\n",
        "weight_decay = 1e-4\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS_AUG = 20\n",
        "\n",
        "# --- CNN 2 blocks + L2 + Augment ---\n",
        "inputs = layers.Input(shape=(32, 32, 3))\n",
        "x = data_augmentation(inputs)\n",
        "\n",
        "x = layers.Conv2D\n"
      ],
      "metadata": {
        "id": "IUlznqZ10Mz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor=\"val_loss\",\n",
        "    factor=0.2,\n",
        "    patience=3,\n",
        "    verbose=1,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "\n",
        "# Callback sencillo para registrar LR por época\n",
        "class LrLogger(tf.keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lrs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        lr = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
        "        self.lrs.append(lr)\n",
        "\n",
        "lr_logger = LrLogger()\n"
      ],
      "metadata": {
        "id": "7lg5rDTU0NSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "cnn_aug_history = cnn_aug_model.fit(\n",
        "    x_train, y_train_oh,\n",
        "    validation_data=(x_valid, y_valid_oh),\n",
        "    epochs=EPOCHS_AUG,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[early_stop, reduce_lr, lr_logger],\n",
        "    verbose=1\n",
        ")\n",
        "total_time_aug = time.time() - start_time\n",
        "time_per_epoch_aug = total_time_aug / len(cnn_aug_history.history[\"loss\"])\n",
        "\n",
        "print(f\"Épocas entrenadas (máx 20): {len(cnn_aug_history.history['loss'])}\")\n",
        "print(f\"Tiempo total: {total_time_aug:.2f} s\")\n",
        "print(f\"Tiempo medio por época: {time_per_epoch_aug:.2f} s\")\n"
      ],
      "metadata": {
        "id": "DGWZtMxG0lky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_aug_val_loss, cnn_aug_val_acc = cnn_aug_model.evaluate(x_valid, y_valid_oh, verbose=0)\n",
        "cnn_aug_test_loss, cnn_aug_test_acc = cnn_aug_model.evaluate(x_test, y_test_oh, verbose=0)\n",
        "\n",
        "print(f\"Validación CNN Aug+L2 - loss: {cnn_aug_val_loss:.4f}, acc: {cnn_aug_val_acc:.4f}\")\n",
        "print(f\"Test CNN Aug+L2       - loss: {cnn_aug_test_loss:.4f}, acc: {cnn_aug_test_acc:.4f}\")\n",
        "\n",
        "cnn_aug_hist_dict = cnn_aug_history.history\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "commit_short_aug = \"rs90ab\"  # pon aquí tu hash corto real\n",
        "\n",
        "# --- Curvas loss/acc ---\n",
        "plt.figure(figsize=(10,4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(cnn_aug_hist_dict[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(cnn_aug_hist_dict[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss CNN 2-blocks Aug+L2+ES+RLROP\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(cnn_aug_hist_dict[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(cnn_aug_hist_dict[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy CNN 2-blocks Aug+L2+ES+RLROP\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "fig_name_aug = f\"{timestamp}_{commit_short_aug}_cnn2blocks_aug_curvas.png\"\n",
        "fig_path_aug = figuras_dir / fig_name_aug\n",
        "plt.savefig(fig_path_aug, dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Figura curvas CNN Aug guardada en:\", fig_path_aug)\n",
        "\n",
        "# --- Curva de LR ---\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.plot(lr_logger.lrs, marker=\"o\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Learning rate\")\n",
        "plt.title(\"Evolución del LR (ReduceLROnPlateau)\")\n",
        "plt.tight_layout()\n",
        "\n",
        "fig_lr_name = f\"{timestamp}_{commit_short_aug}_lr_schedule.png\"\n",
        "fig_lr_path = figuras_dir / fig_lr_name\n",
        "plt.savefig(fig_lr_path, dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Figura LR guardada en:\", fig_lr_path)\n"
      ],
      "metadata": {
        "id": "CMahPR_30oKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Elegimos una imagen de ejemplo\n",
        "idx = 0\n",
        "sample_img = x_train[idx]  # ya está en [0,1]\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "for i in range(9):\n",
        "    augmented = data_augmentation(\n",
        "        tf.expand_dims(sample_img, axis=0),\n",
        "        training=True\n",
        "    )\n",
        "    aug_img = augmented[0].numpy().clip(0,1)\n",
        "\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(aug_img)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "fig_aug_samples_name = f\"{timestamp}_{commit_short_aug}_aug_samples.png\"\n",
        "fig_aug_samples_path = figuras_dir / fig_aug_samples_name\n",
        "plt.savefig(fig_aug_samples_path, dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Figura de muestras de augmentation guardada en:\", fig_aug_samples_path)\n"
      ],
      "metadata": {
        "id": "yiQlwrjB0qDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# History\n",
        "cnn_aug_history_df = pd.DataFrame(cnn_aug_hist_dict)\n",
        "cnn_aug_history_df[\"lr\"] = lr_logger.lrs  # guardamos también LR por época\n",
        "\n",
        "cnn_aug_history_csv_path = results_dir / \"history_cnn_2blocks_aug_l2_es_rlrop.csv\"\n",
        "cnn_aug_history_df.to_csv(cnn_aug_history_csv_path, index=False)\n",
        "print(\"History CNN Aug guardado en:\", cnn_aug_history_csv_path)\n",
        "\n",
        "# Métricas\n",
        "metrics_path = results_dir / \"metrics.json\"\n",
        "\n",
        "cnn_aug_metrics = {\n",
        "    \"model\": \"cnn_2blocks_l2_aug_rlrop\",\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"val_loss\": float(cnn_aug_val_loss),\n",
        "    \"val_acc\": float(cnn_aug_val_acc),\n",
        "    \"test_loss\": float(cnn_aug_test_loss),\n",
        "    \"test_acc\": float(cnn_aug_test_acc),\n",
        "    \"time_per_epoch\": float(time_per_epoch_aug),\n",
        "    \"params\": int(cnn_aug_model.count_params()),\n",
        "    \"l2\": float(weight_decay),\n",
        "    \"augmentation\": {\n",
        "        \"flip_horizontal\": True,\n",
        "        \"rotation\": 0.10,\n",
        "        \"zoom\": 0.10,\n",
        "        \"translation\": [0.10, 0.10]\n",
        "    },\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"ReduceLROnPlateau\",\n",
        "        \"monitor\": \"val_loss\",\n",
        "        \"factor\": 0.2,\n",
        "        \"patience\": 3,\n",
        "        \"min_lr\": 1e-6\n",
        "    },\n",
        "    \"early_stopping\": {\n",
        "        \"monitor\": \"val_loss\",\n",
        "        \"patience\": 5,\n",
        "        \"restore_best_weights\": True,\n",
        "        \"epochs_trained\": len(cnn_aug_hist_dict[\"loss\"])\n",
        "    },\n",
        "    \"commit\": commit_short_aug\n",
        "}\n",
        "\n",
        "if metrics_path.exists():\n",
        "    with open(metrics_path, \"r\") as f:\n",
        "        try:\n",
        "            metrics_list = json.load(f)\n",
        "            if not isinstance(metrics_list, list):\n",
        "                metrics_list = [metrics_list]\n",
        "        except json.JSONDecodeError:\n",
        "            metrics_list = []\n",
        "else:\n",
        "    metrics_list = []\n",
        "\n",
        "metrics_list.append(cnn_aug_metrics)\n",
        "\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    json.dump(metrics_list, f, indent=4)\n",
        "\n",
        "print(\"metrics.json actualizado con CNN Aug:\", metrics_path)\n"
      ],
      "metadata": {
        "id": "FVsV24Qw0ulh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_path = results_dir / \"params.yaml\"\n",
        "\n",
        "if params_path.exists():\n",
        "    with open(params_path, \"r\") as f:\n",
        "        params = yaml.safe_load(f)\n",
        "else:\n",
        "    params = {}\n",
        "\n",
        "params[\"cnn_2blocks_l2_aug_rlrop\"] = {\n",
        "    \"blocks\": 2,\n",
        "    \"filters\": [32, 64],\n",
        "    \"kernel_size\": 3,\n",
        "    \"dense_units\": 128,\n",
        "    \"dropout\": 0.5,\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"max_epochs\": EPOCHS_AUG,\n",
        "    \"seed\": 42,\n",
        "    \"l2\": weight_decay,\n",
        "    \"augmentation\": {\n",
        "        \"flip_horizontal\": True,\n",
        "        \"rotation\": 0.10,\n",
        "        \"zoom\": 0.10,\n",
        "        \"translation\": [0.10, 0.10]\n",
        "    },\n",
        "    \"scheduler\": \"ReduceLROnPlateau\",\n",
        "    \"early_stopping\": {\n",
        "        \"monitor\": \"val_loss\",\n",
        "        \"patience\": 5,\n",
        "        \"restore_best_weights\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(params_path, \"w\") as f:\n",
        "    yaml.dump(params, f, sort_keys=False)\n",
        "\n",
        "print(\"params.yaml actualizado con CNN Aug+RLROP en:\", params_path)\n"
      ],
      "metadata": {
        "id": "ShIqRq6C00Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Resumen CNN Aug:\", cnn_aug_summary_path.exists())\n",
        "print(\"History Aug CSV:\", cnn_aug_history_csv_path.exists())\n",
        "print(\"Metrics JSON:\", metrics_path.exists())\n",
        "print(\"Figura curvas:\", fig_path_aug.exists())\n",
        "print(\"Figura LR:\", fig_lr_path.exists())\n",
        "print(\"Figura muestras augmentation:\", fig_aug_samples_path.exists())\n",
        "\n",
        "print(\"Épocas entrenadas (<=20):\", len(cnn_aug_hist_dict[\"loss\"]))\n",
        "print(\"Mínima val_loss:\", min(cnn_aug_hist_dict[\"val_loss\"]))\n",
        "print(\"LRs por época:\", lr_logger.lrs)\n"
      ],
      "metadata": {
        "id": "8SFgKqYS026S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Mejoró la test accuracy?\n",
        "Con el Data Augmentation moderado y el scheduler ReduceLROnPlateau, la test_acc suele subir algo respecto al modelo con solo L2+EarlyStopping, especialmente si el dataset original era algo justo para la capacidad de la red. El augmentation genera variantes razonables de las imágenes (flips, rotaciones pequeñas, zoom y traslación), lo que fuerza al modelo a aprender características más invariantes y menos dependientes de la posición exacta o del ruido específico del conjunto de entrenamiento.\n",
        "¿Cómo afectó a la convergencia?\n",
        "El entrenamiento se vuelve algo más ruidoso y lento por época (se aplican transformaciones en cada batch), y las curvas de loss/accuracy convergen de forma más suave. ReduceLROnPlateau baja la tasa de aprendizaje cuando val_loss se estanca, lo que se ve en la curva de LR: tras varios epochs sin mejora clara, el LR cae y la loss de validación puede seguir refinándose. En resumen, la convergencia es menos “rápida” al principio, pero más estable al final, y el modelo final tiende a generalizar mejor aunque tarde algún epoch extra en llegar a su mejor punto.\n"
      ],
      "metadata": {
        "id": "V4L1hmxo054E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import time, json, yaml\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Mismos identificadores que en los prompts anteriores\n",
        "APELLIDO = \"Relloso\"   # cámbialo\n",
        "NOMBRE = \"Daniel\"       # cámbialo\n",
        "REPO_NAME = f\"IA_P3_CIFAR10_{APELLIDO}{NOMBRE}\"\n",
        "\n",
        "base_dir    = Path(REPO_NAME)\n",
        "results_dir = base_dir / \"results\"\n",
        "figuras_dir = base_dir / \"figuras\"\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "figuras_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Reutilizamos un augment moderado (puedes mantener el del prompt 5)\n",
        "data_augmentation_3b = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.10),\n",
        "    layers.RandomZoom(0.10),\n",
        "    layers.RandomTranslation(0.10, 0.10)\n",
        "], name=\"data_augmentation_3blocks\")\n",
        "\n",
        "weight_decay = 1e-4\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS_3BLOCKS = 30\n",
        "\n",
        "# --- CNN 3 bloques: 32 -> 64 -> 128 ---\n",
        "inputs = layers.Input(shape=(32, 32, 3))\n",
        "x = data_augmentation_3b(inputs)\n",
        "\n",
        "# Bloque 1: 32 filtros\n",
        "x = layers.Conv2D(\n",
        "    32, (3, 3), activation=\"relu\", padding=\"same\",\n",
        "    kernel_regularizer=regularizers.l2(weight_decay)\n",
        ")(x)\n",
        "x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Bloque 2: 64 filtros\n",
        "x = layers.Conv2D(\n",
        "    64, (3, 3), activation=\"relu\", padding=\"same\",\n",
        "    kernel_regularizer=regularizers.l2(weight_decay)\n",
        ")(x)\n",
        "x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Bloque 3: 128 filtros\n",
        "x = layers.Conv2D(\n",
        "    128, (3, 3), activation=\"relu\", padding=\"same\",\n",
        "    kernel_regularizer=regularizers.l2(weight_decay)\n",
        ")(x)\n",
        "x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Cabeza densa\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(\n",
        "    128, activation=\"relu\",\n",
        "    kernel_regularizer=regularizers.l2(weight_decay)\n",
        ")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "\n",
        "cnn_3b_model = models.Model(inputs=inputs, outputs=outputs, name=\"cnn_3blocks_l2_aug\")\n",
        "\n",
        "cnn_3b_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Guardar summary\n",
        "cnn_3b_summary_path = results_dir / \"cnn_3blocks_l2_aug_rlrop_summary.txt\"\n",
        "with open(cnn_3b_summary_path, \"w\") as f:\n",
        "    cnn_3b_model.summary(print_fn=lambda l: f.write(l + \"\\n\"))\n",
        "\n",
        "cnn_3b_model.summary()\n",
        "print(\"Resumen CNN 3-blocks guardado en:\", cnn_3b_summary_path)\n",
        "print(\"Parámetros CNN 3-blocks:\", cnn_3b_model.count_params())\n"
      ],
      "metadata": {
        "id": "eNT8oLGh06b2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop_3b = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr_3b = ReduceLROnPlateau(\n",
        "    monitor=\"val_loss\",\n",
        "    factor=0.2,\n",
        "    patience=3,\n",
        "    verbose=1,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "\n",
        "class LrLogger(tf.keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lrs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        lr = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
        "        self.lrs.append(lr)\n",
        "\n",
        "lr_logger_3b = LrLogger()\n"
      ],
      "metadata": {
        "id": "DS-IjYmy1Lzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time_3b = time.time()\n",
        "cnn_3b_history = cnn_3b_model.fit(\n",
        "    x_train, y_train_oh,\n",
        "    validation_data=(x_valid, y_valid_oh),\n",
        "    epochs=EPOCHS_3BLOCKS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[early_stop_3b, reduce_lr_3b, lr_logger_3b],\n",
        "    verbose=1\n",
        ")\n",
        "total_time_3b = time.time() - start_time_3b\n",
        "time_per_epoch_3b = total_time_3b / len(cnn_3b_history.history[\"loss\"])\n",
        "\n",
        "print(f\"Épocas entrenadas (máx 30): {len(cnn_3b_history.history['loss'])}\")\n",
        "print(f\"Tiempo total 3-blocks: {total_time_3b:.2f} s\")\n",
        "print(f\"Tiempo medio por época 3-blocks: {time_per_epoch_3b:.2f} s\")\n"
      ],
      "metadata": {
        "id": "-zaNKbwY1NbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_3b_val_loss, cnn_3b_val_acc = cnn_3b_model.evaluate(x_valid, y_valid_oh, verbose=0)\n",
        "cnn_3b_test_loss, cnn_3b_test_acc = cnn_3b_model.evaluate(x_test, y_test_oh, verbose=0)\n",
        "\n",
        "print(f\"Validación CNN 3-blocks - loss: {cnn_3b_val_loss:.4f}, acc: {cnn_3b_val_acc:.4f}\")\n",
        "print(f\"Test CNN 3-blocks       - loss: {cnn_3b_test_loss:.4f}, acc: {cnn_3b_test_acc:.4f}\")\n",
        "\n",
        "cnn_3b_hist_dict = cnn_3b_history.history\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "commit_short_3b = \"tu12h3\"  # pon aquí el hash corto real del commit\n",
        "\n",
        "# --- Curvas loss/acc ---\n",
        "plt.figure(figsize=(10,4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(cnn_3b_hist_dict[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(cnn_3b_hist_dict[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss CNN 3-blocks L2+Aug+ES+RLROP\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(cnn_3b_hist_dict[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(cnn_3b_hist_dict[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy CNN 3-blocks L2+Aug+ES+RLROP\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "fig_name_3b = f\"{timestamp}_{commit_short_3b}_cnn3blocks_curvas.png\"\n",
        "fig_path_3b = figuras_dir / fig_name_3b\n",
        "plt.savefig(fig_path_3b, dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Figura curvas CNN 3-blocks guardada en:\", fig_path_3b)\n",
        "\n",
        "# --- Curva de LR ---\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.plot(lr_logger_3b.lrs, marker=\"o\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Learning rate\")\n",
        "plt.title(\"LR schedule CNN 3-blocks\")\n",
        "plt.tight_layout()\n",
        "\n",
        "fig_lr_3b_name = f\"{timestamp}_{commit_short_3b}_cnn3blocks_lr.png\"\n",
        "fig_lr_3b_path = figuras_dir / fig_lr_3b_name\n",
        "plt.savefig(fig_lr_3b_path, dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Figura LR CNN 3-blocks guardada en:\", fig_lr_3b_path)\n"
      ],
      "metadata": {
        "id": "Lul-cnyc1OjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# History\n",
        "cnn_3b_history_df = pd.DataFrame(cnn_3b_hist_dict)\n",
        "cnn_3b_history_df[\"lr\"] = lr_logger_3b.lrs\n",
        "\n",
        "cnn_3b_history_csv_path = results_dir / \"history_cnn_3blocks_l2_aug_rlrop.csv\"\n",
        "cnn_3b_history_df.to_csv(cnn_3b_history_csv_path, index=False)\n",
        "print(\"History CNN 3-blocks guardado en:\", cnn_3b_history_csv_path)\n",
        "\n",
        "# Métricas\n",
        "metrics_path = results_dir / \"metrics.json\"\n",
        "\n",
        "cnn_3b_metrics = {\n",
        "    \"model\": \"cnn_3blocks_l2_aug_rlrop\",\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"val_loss\": float(cnn_3b_val_loss),\n",
        "    \"val_acc\": float(cnn_3b_val_acc),\n",
        "    \"test_loss\": float(cnn_3b_test_loss),\n",
        "    \"test_acc\": float(cnn_3b_test_acc),\n",
        "    \"time_per_epoch\": float(time_per_epoch_3b),\n",
        "    \"params\": int(cnn_3b_model.count_params()),\n",
        "    \"l2\": float(weight_decay),\n",
        "    \"augmentation\": {\n",
        "        \"flip_horizontal\": True,\n",
        "        \"rotation\": 0.10,\n",
        "        \"zoom\": 0.10,\n",
        "        \"translation\": [0.10, 0.10]\n",
        "    },\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"ReduceLROnPlateau\",\n",
        "        \"monitor\": \"val_loss\",\n",
        "        \"factor\": 0.2,\n",
        "        \"patience\": 3,\n",
        "        \"min_lr\": 1e-6\n",
        "    },\n",
        "    \"early_stopping\": {\n",
        "        \"monitor\": \"val_loss\",\n",
        "        \"patience\": 5,\n",
        "        \"restore_best_weights\": True,\n",
        "        \"epochs_trained\": len(cnn_3b_hist_dict[\"loss\"])\n",
        "    },\n",
        "    \"commit\": commit_short_3b\n",
        "}\n",
        "\n",
        "if metrics_path.exists():\n",
        "    with open(metrics_path, \"r\") as f:\n",
        "        try:\n",
        "            metrics_list = json.load(f)\n",
        "            if not isinstance(metrics_list, list):\n",
        "                metrics_list = [metrics_list]\n",
        "        except json.JSONDecodeError:\n",
        "            metrics_list = []\n",
        "else:\n",
        "    metrics_list = []\n",
        "\n",
        "metrics_list.append(cnn_3b_metrics)\n",
        "\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    json.dump(metrics_list, f, indent=4)\n",
        "\n",
        "print(\"metrics.json actualizado con CNN 3-blocks:\", metrics_path)\n"
      ],
      "metadata": {
        "id": "PFw-frOc1Qym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_path = results_dir / \"params.yaml\"\n",
        "\n",
        "if params_path.exists():\n",
        "    with open(params_path, \"r\") as f:\n",
        "        params = yaml.safe_load(f)\n",
        "else:\n",
        "    params = {}\n",
        "\n",
        "params[\"cnn_3blocks_l2_aug_rlrop\"] = {\n",
        "    \"blocks\": 3,\n",
        "    \"filters\": [32, 64, 128],\n",
        "    \"kernel_size\": 3,\n",
        "    \"dense_units\": 128,\n",
        "    \"dropout\": 0.5,\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"max_epochs\": EPOCHS_3BLOCKS,\n",
        "    \"seed\": 42,\n",
        "    \"l2\": weight_decay,\n",
        "    \"augmentation\": {\n",
        "        \"flip_horizontal\": True,\n",
        "        \"rotation\": 0.10,\n",
        "        \"zoom\": 0.10,\n",
        "        \"translation\": [0.10, 0.10]\n",
        "    },\n",
        "    \"scheduler\": \"ReduceLROnPlateau\",\n",
        "    \"early_stopping\": {\n",
        "        \"monitor\": \"val_loss\",\n",
        "        \"patience\": 5,\n",
        "        \"restore_best_weights\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(params_path, \"w\") as f:\n",
        "    yaml.dump(params, f, sort_keys=False)\n",
        "\n",
        "print(\"params.yaml actualizado con CNN 3-blocks en:\", params_path)\n",
        ""
      ],
      "metadata": {
        "id": "xhukBICs1S_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparacion_path = results_dir / \"comparacion_mlp_vs_cnn.csv\"\n",
        "\n",
        "# Leemos la tabla si existe\n",
        "if comparacion_path.exists():\n",
        "    comparacion_df = pd.read_csv(comparacion_path)\n",
        "else:\n",
        "    comparacion_df = pd.DataFrame(columns=[\"modelo\", \"params\", \"time_per_epoch_s\", \"val_acc\", \"test_acc\"])\n",
        "\n",
        "new_row_3b = {\n",
        "    \"modelo\": \"CNN 3-blocks\",\n",
        "    \"params\": int(cnn_3b_model.count_params()),\n",
        "    \"time_per_epoch_s\": float(time_per_epoch_3b),\n",
        "    \"val_acc\": float(cnn_3b_val_acc),\n",
        "    \"test_acc\": float(cnn_3b_test_acc)\n",
        "}\n",
        "\n",
        "comparacion_df = pd.concat(\n",
        "    [comparacion_df, pd.DataFrame([new_row_3b])],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "comparacion_df.to_csv(comparacion_path, index=False)\n",
        "\n",
        "print(comparacion_df)\n",
        "print(\"Tabla comparativa actualizada en:\", comparacion_path)\n"
      ],
      "metadata": {
        "id": "Z_3PUggG1Y2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Summary 3-blocks:\", cnn_3b_summary_path.exists())\n",
        "print(\"History 3-blocks CSV:\", cnn_3b_history_csv_path.exists())\n",
        "print(\"Metrics JSON:\", metrics_path.exists())\n",
        "print(\"Figura curvas 3-blocks:\", fig_path_3b.exists())\n",
        "print(\"Figura LR 3-blocks:\", fig_lr_3b_path.exists())\n",
        "print(\"Comparación:\", comparacion_path.exists())\n",
        "print(\"Épocas entrenadas 3-blocks:\", len(cnn_3b_hist_dict[\"loss\"]))\n",
        "print(\"Parámetros CNN 3-blocks:\", cnn_3b_model.count_params())\n"
      ],
      "metadata": {
        "id": "Nd0I3UjL1abA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Compensa el salto de capacidad en métricas y coste?\n",
        "Al pasar de la CNN de 2 bloques a la de 3 bloques (32→64→128 filtros) el número de parámetros y el tiempo por época aumentan de forma clara. La red ve patrones más complejos y profundos, lo que suele traducirse en una ligera mejora de val_acc y test_acc, sobre todo si el modelo anterior se quedaba algo corto de capacidad. Sin embargo, el incremento no siempre es “proporcional” al coste: muchas veces se gana solo unos pocos puntos de accuracy a cambio de más memoria, más FLOPs y épocas algo más largas.\n",
        "Con L2, Data Augmentation, EarlyStopping y ReduceLROnPlateau, el modelo profundo se mantiene relativamente controlado en sobreajuste, pero se vuelve más sensible al tuning (LR, regularización, etc.). En resumen: si tus recursos y la práctica lo permiten, el 3-blocks puede justificar el coste si de verdad mejora la métrica en valid/test; si la mejora es marginal respecto a la 2-blocks, probablemente la CNN de 2 bloques sea un mejor compromiso entre rendimiento y eficiencia para CIFAR-10 en este contexto de práctica."
      ],
      "metadata": {
        "id": "yqWmrdt71cAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import json, yaml\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Ajusta estos datos si tu mejor modelo es otro\n",
        "BEST_MODEL_NAME = \"cnn_3blocks_l2_aug_rlrop\"\n",
        "best_model = cnn_3b_model              # o cnn_aug_model, etc.\n",
        "best_val_acc = float(cnn_3b_val_acc)\n",
        "best_test_acc = float(cnn_3b_test_acc)\n",
        "\n",
        "# Usa el mismo hash corto que en el commit del modelo 3-block\n",
        "commit_best = commit_short_3b          # asegúrate de que esta variable existe\n",
        "\n",
        "APELLIDO = \"Relloso\"\n",
        "NOMBRE = \"Daniel\"\n",
        "REPO_NAME = f\"IA_P3_CIFAR10_{APELLIDO}{NOMBRE}\"\n",
        "\n",
        "base_dir    = Path(REPO_NAME)\n",
        "results_dir = base_dir / \"results\"\n",
        "figuras_dir = base_dir / \"figuras\"\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "figuras_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Nombres de clase CIFAR-10\n",
        "class_names = [\n",
        "    \"airplane\", \"automobile\", \"bird\", \"cat\",\n",
        "    \"deer\", \"dog\", \"frog\", \"horse\",\n",
        "    \"ship\", \"truck\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "Hm5xeAdC1cZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_test lo tienes de la carga original: (10000, 1)\n",
        "y_true = y_test.ravel()\n",
        "\n",
        "y_proba = best_model.predict(x_test, batch_size=64, verbose=1)\n",
        "y_pred = np.argmax(y_proba, axis=1)\n",
        "\n",
        "print(\"Shapes -> y_true:\", y_true.shape, \"y_pred:\", y_pred.shape)\n",
        "\n",
        "# Matriz de confusión\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.imshow(cm, interpolation=\"nearest\")\n",
        "plt.title(f\"Confusion matrix - {BEST_MODEL_NAME}\")\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(class_names))\n",
        "plt.xticks(tick_marks, class_names, rotation=45, ha=\"right\")\n",
        "plt.yticks(tick_marks, class_names)\n",
        "\n",
        "plt.ylabel(\"Real\")\n",
        "plt.xlabel(\"Predicho\")\n",
        "plt.tight_layout()\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "fig_cm_name = f\"{timestamp}_{commit_best}_confusion_matrix_{BEST_MODEL_NAME}.png\"\n",
        "fig_cm_path = figuras_dir / fig_cm_name\n",
        "plt.savefig(fig_cm_path, dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Matriz de confusión guardada en:\", fig_cm_path)\n"
      ],
      "metadata": {
        "id": "FACUJlWb18aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Índices donde el modelo falla\n",
        "error_indices = np.where(y_true != y_pred)[0]\n",
        "print(\"Número de errores:\", len(error_indices))\n",
        "\n",
        "# Tomamos hasta 12 ejemplos\n",
        "num_errors_to_show = min(12, len(error_indices))\n",
        "selected_errors = np.random.choice(error_indices, size=num_errors_to_show, replace=False)\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "for i, idx in enumerate(selected_errors):\n",
        "    plt.subplot(3, 4, i+1)\n",
        "    plt.imshow(x_test[idx])\n",
        "    plt.axis(\"off\")\n",
        "    real_label = class_names[y_true[idx]]\n",
        "    pred_label = class_names[y_pred[idx]]\n",
        "    plt.title(f\"Real: {real_label}\\nPred: {pred_label}\", fontsize=9)\n",
        "plt.tight_layout()\n",
        "\n",
        "fig_errors_name = f\"{timestamp}_{commit_best}_errors_{BEST_MODEL_NAME}.png\"\n",
        "fig_errors_path = figuras_dir / fig_errors_name\n",
        "plt.savefig(fig_errors_path, dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Figura de errores guardada en:\", fig_errors_path)\n"
      ],
      "metadata": {
        "id": "izlfR8-71-H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figuras_table_path = results_dir / \"figuras.csv\"\n",
        "\n",
        "figuras_rows = [\n",
        "    {\n",
        "        \"fecha\": timestamp,\n",
        "        \"modelo\": BEST_MODEL_NAME,\n",
        "        \"commit\": commit_best,\n",
        "        \"tipo\": \"confusion_matrix\",\n",
        "        \"archivo\": fig_cm_name,\n",
        "        \"descripcion\": \"Matriz de confusión en test para el mejor modelo\"\n",
        "    },\n",
        "    {\n",
        "        \"fecha\": timestamp,\n",
        "        \"modelo\": BEST_MODEL_NAME,\n",
        "        \"commit\": commit_best,\n",
        "        \"tipo\": \"error_examples\",\n",
        "        \"archivo\": fig_errors_name,\n",
        "        \"descripcion\": \"12 ejemplos mal clasificados (Real vs Predicho)\"\n",
        "    }\n",
        "]\n",
        "\n",
        "if figuras_table_path.exists():\n",
        "    figuras_df = pd.read_csv(figuras_table_path)\n",
        "    figuras_df = pd.concat([figuras_df, pd.DataFrame(figuras_rows)], ignore_index=True)\n",
        "else:\n",
        "    figuras_df = pd.DataFrame(figuras_rows)\n",
        "\n",
        "figuras_df.to_csv(figuras_table_path, index=False)\n",
        "print(\"Tabla de figuras actualizada en:\", figuras_table_path)\n",
        "print(figuras_df.tail(5))\n"
      ],
      "metadata": {
        "id": "4l71Pfbf1_v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_path = results_dir / \"params.yaml\"\n",
        "\n",
        "if params_path.exists():\n",
        "    with open(params_path, \"r\") as f:\n",
        "        params = yaml.safe_load(f)\n",
        "else:\n",
        "    params = {}\n",
        "\n",
        "params[\"best_model\"] = {\n",
        "    \"name\": BEST_MODEL_NAME,\n",
        "    \"commit\": commit_best,\n",
        "    \"val_acc\": best_val_acc,\n",
        "    \"test_acc\": best_test_acc,\n",
        "    \"fecha\": timestamp,\n",
        "    \"criterio\": \"Seleccionado por mayor test_acc y buen equilibrio train/val\"\n",
        "}\n",
        "\n",
        "with open(params_path, \"w\") as f:\n",
        "    yaml.dump(params, f, sort_keys=False)\n",
        "\n",
        "print(\"params.yaml actualizado con best_model en:\", params_path)\n"
      ],
      "metadata": {
        "id": "Kot6Z8QP2CLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_path = results_dir / \"metrics.json\"\n",
        "\n",
        "if metrics_path.exists():\n",
        "    with open(metrics_path, \"r\") as f:\n",
        "        metrics_list = json.load(f)\n",
        "        if not isinstance(metrics_list, list):\n",
        "            metrics_list = [metrics_list]\n",
        "else:\n",
        "    metrics_list = []\n",
        "\n",
        "# Quitar el flag 'best' de todos los anteriores\n",
        "for m in metrics_list:\n",
        "    if \"best\" in m:\n",
        "        m.pop(\"best\")\n",
        "\n",
        "# Buscar la entrada del mejor modelo por nombre y (opcionalmente) commit\n",
        "found = False\n",
        "for m in metrics_list:\n",
        "    if m.get(\"model\") == BEST_MODEL_NAME and m.get(\"commit\") == commit_best:\n",
        "        m[\"best\"] = True\n",
        "        found = True\n",
        "        # Actualizamos también por claridad\n",
        "        m[\"best_test_acc\"] = best_test_acc\n",
        "        break\n",
        "\n",
        "# Si no la encuentra (por si has cambiado nombres), añade un registro nuevo\n",
        "if not found:\n",
        "    metrics_list.append({\n",
        "        \"model\": BEST_MODEL_NAME,\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"val_acc\": best_val_acc,\n",
        "        \"test_acc\": best_test_acc,\n",
        "        \"best\": True,\n",
        "        \"best_test_acc\": best_test_acc,\n",
        "        \"commit\": commit_best\n",
        "    })\n",
        "\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    json.dump(metrics_list, f, indent=4)\n",
        "\n",
        "print(\"metrics.json actualizado con best_model. Total entradas:\", len(metrics_list))\n"
      ],
      "metadata": {
        "id": "_CmMSiIp2Dah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Figura matriz confusión existe:\", fig_cm_path.exists())\n",
        "print(\"Figura errores existe:\", fig_errors_path.exists())\n",
        "print(\"Tabla figuras existe:\", figuras_table_path.exists())\n",
        "print(\"params.yaml existe:\", params_path.exists())\n",
        "print(\"metrics.json existe:\", metrics_path.exists())\n",
        ""
      ],
      "metadata": {
        "id": "gv-w0cm_2Ew1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo considerado “mejor”\n",
        "He tomado como mejor modelo la CNN 3-blocks con L2 + Data Augmentation + EarlyStopping + ReduceLROnPlateau (cnn_3blocks_l2_aug_rlrop), porque es el que obtiene la mayor test accuracy con una brecha train/val razonable y un comportamiento estable en validación.\n",
        "Pares de clases más confundidos\n",
        "En la matriz de confusión se observa que las confusiones más frecuentes se dan entre pares visualmente parecidos, como cat vs dog, deer vs horse o automobile vs truck. También pueden aparecer confusiones puntuales entre clases con fondos similares (por ejemplo, animales en entorno natural) donde el modelo aún depende demasiado del contexto y no tanto de la forma del objeto.\n",
        "Mejoras concretas posibles\n",
        "\n",
        "\n",
        "Augment dirigido: reforzar el Data Augmentation sobre las clases que peor se comportan (más rotaciones leves, traslaciones y variaciones de brillo/contraste en gatos/perros, coches/camiones, etc.), para que el modelo vea más variaciones de esas categorías.\n",
        "\n",
        "\n",
        "Label smoothing: añadir un label smoothing suave (por ejemplo 0.1) para evitar que el modelo se vuelva demasiado “seguro” en una única clase y mejorar la calibración en clases muy parecidas.\n",
        "\n",
        "\n",
        "Rebalancear minibatches: garantizar que cada batch contenga suficientes ejemplos de las clases más difíciles, aumentando su presencia en el entrenamiento efectivo.\n",
        "\n",
        "\n",
        "Arquitectura/aug extra fino: se podría añadir un bloque convolucional adicional ligero o usar kernels algo más grandes en las últimas capas para capturar patrones más globales, aunque siempre evaluando si la mejora compensa el coste extra.\n",
        "\n",
        "\n",
        "En conjunto, la matriz de confusión confirma que el modelo ya aprende bien la mayoría de clases, y que las mejoras pasan por atacar específicamente las clases más confundidas con augment más inteligente y regularización suave en la pérdida.\n"
      ],
      "metadata": {
        "id": "0aNgwSmC2Gvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import time, json, yaml\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Identificadores\n",
        "APELLIDO = \"Relloso\"   # cámbialo\n",
        "NOMBRE = \"Daniel\"       # cámbialo\n",
        "REPO_NAME = f\"IA_P3_CIFAR10_{APELLIDO}{NOMBRE}\"\n",
        "\n",
        "base_dir    = Path(REPO_NAME)\n",
        "results_dir = base_dir / \"results\"\n",
        "figuras_dir = base_dir / \"figuras\"\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "figuras_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Config\n",
        "weight_decay = 1e-4\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS_SGD = 30\n",
        "\n",
        "# Data augmentation (igual que en el mejor modelo)\n",
        "data_augmentation_sgd = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.10),\n",
        "    layers.RandomZoom(0.10),\n",
        "    layers.RandomTranslation(0.10, 0.10)\n",
        "], name=\"data_augmentation_sgd\")\n",
        "\n",
        "# --- Scheduler CosineDecay (por step) ---\n",
        "steps_per_epoch = int(np.ceil(x_train.shape[0] / BATCH_SIZE))\n",
        "decay_steps = steps_per_epoch * EPOCHS_SGD\n",
        "\n",
        "cosine_lr = tf.keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate=0.05,\n",
        "    decay_steps=decay_steps,\n",
        "    alpha=0.0  # hacia ~0 al final\n",
        ")\n",
        "\n",
        "# --- Definir la misma arquitectura 3-blocks pero con SGD+CosineDecay ---\n",
        "inputs = layers.Input(shape=(32, 32, 3))\n",
        "x = data_augmentation_sgd(inputs)\n",
        "\n",
        "x = layers.Conv2D(\n",
        "    32, (3, 3), activation=\"relu\", padding=\"same\",\n",
        "    kernel_regularizer=regularizers.l2(weight_decay)\n",
        ")(x)\n",
        "x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "x = layers.Conv2D(\n",
        "    64, (3, 3), activation=\"relu\", padding=\"same\",\n",
        "    kernel_regularizer=regularizers.l2(weight_decay)\n",
        ")(x)\n",
        "x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "x = layers.Conv2D(\n",
        "    128, (3, 3), activation=\"relu\", padding=\"same\",\n",
        "    kernel_regularizer=regularizers.l2(weight_decay)\n",
        ")(x)\n",
        "x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(\n",
        "    128, activation=\"relu\",\n",
        "    kernel_regularizer=regularizers.l2(weight_decay)\n",
        ")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "\n",
        "cnn_3b_sgd_model = models.Model(inputs=inputs, outputs=outputs, name=\"cnn_3blocks_l2_aug_sgd_cosine\")\n",
        "\n",
        "optimizer_sgd = tf.keras.optimizers.SGD(\n",
        "    learning_rate=cosine_lr,\n",
        "    momentum=0.9\n",
        ")\n",
        "\n",
        "cnn_3b_sgd_model.compile(\n",
        "    optimizer=optimizer_sgd,\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Guardar summary\n",
        "cnn_3b_sgd_summary_path = results_dir / \"cnn_3blocks_l2_aug_sgd_cosine_summary.txt\"\n",
        "with open(cnn_3b_sgd_summary_path, \"w\") as f:\n",
        "    cnn_3b_sgd_model.summary(print_fn=lambda l: f.write(l + \"\\n\"))\n",
        "\n",
        "cnn_3b_sgd_model.summary()\n",
        "print(\"Resumen CNN 3-blocks SGD+Cosine guardado en:\", cnn_3b_sgd_summary_path)\n",
        "print(\"Parámetros CNN 3-blocks SGD+Cosine:\", cnn_3b_sgd_model.count_params())\n"
      ],
      "metadata": {
        "id": "olxBGq_p2HGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "early_stop_sgd = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "class EpochLrLogger(Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lrs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        lr = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
        "        self.lrs.append(lr)\n",
        "        print(f\"Epoch {epoch+1} - LR: {lr:.6f}\")\n",
        "\n",
        "lr_logger_sgd = EpochLrLogger()\n"
      ],
      "metadata": {
        "id": "Ygi3hLkY2fbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time_sgd = time.time()\n",
        "cnn_3b_sgd_history = cnn_3b_sgd_model.fit(\n",
        "    x_train, y_train_oh,\n",
        "    validation_data=(x_valid, y_valid_oh),\n",
        "    epochs=EPOCHS_SGD,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[early_stop_sgd, lr_logger_sgd],\n",
        "    verbose=1\n",
        ")\n",
        "total_time_sgd = time.time() - start_time_sgd\n",
        "time_per_epoch_sgd = total_time_sgd / len(cnn_3b_sgd_history.history[\"loss\"])\n",
        "\n",
        "print(f\"Épocas entrenadas (máx 30): {len(cnn_3b_sgd_history.history['loss'])}\")\n",
        "print(f\"Tiempo total SGD+Cosine: {total_time_sgd:.2f} s\")\n",
        "print(f\"Tiempo medio por época SGD+Cosine: {time_per_epoch_sgd:.2f} s\")\n",
        "\n",
        "print(\"LR por época (CosineDecay):\")\n",
        "print(lr_logger_sgd.lrs)\n"
      ],
      "metadata": {
        "id": "3Oa8_bqd2hej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_3b_sgd_val_loss, cnn_3b_sgd_val_acc = cnn_3b_sgd_model.evaluate(x_valid, y_valid_oh, verbose=0)\n",
        "cnn_3b_sgd_test_loss, cnn_3b_sgd_test_acc = cnn_3b_sgd_model.evaluate(x_test, y_test_oh, verbose=0)\n",
        "\n",
        "print(f\"Validación CNN 3-blocks SGD+Cosine - loss: {cnn_3b_sgd_val_loss:.4f}, acc: {cnn_3b_sgd_val_acc:.4f}\")\n",
        "print(f\"Test CNN 3-blocks SGD+Cosine       - loss: {cnn_3b_sgd_test_loss:.4f}, acc: {cnn_3b_sgd_test_acc:.4f}\")\n",
        "\n",
        "cnn_3b_sgd_hist_dict = cnn_3b_sgd_history.history\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "commit_short_sgd = \"sgd123\"  # pon aquí tu hash corto real\n",
        "\n",
        "# --- Curvas solo del entrenamiento SGD ---\n",
        "plt.figure(figsize=(10,4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(cnn_3b_sgd_hist_dict[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(cnn_3b_sgd_hist_dict[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss CNN 3-blocks SGD+Cosine\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(cnn_3b_sgd_hist_dict[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(cnn_3b_sgd_hist_dict[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy CNN 3-blocks SGD+Cosine\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "fig_sgd_curvas_name = f\"{timestamp}_{commit_short_sgd}_cnn3blocks_sgd_cosine_curvas.png\"\n",
        "fig_sgd_curvas_path = figuras_dir / fig_sgd_curvas_name\n",
        "plt.savefig(fig_sgd_curvas_path, dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Figura curvas SGD+Cosine guardada en:\", fig_sgd_curvas_path)\n",
        "\n",
        "# --- Curva de LR ---\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.plot(lr_logger_sgd.lrs, marker=\"o\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Learning rate\")\n",
        "plt.title(\"LR por época (CosineDecay)\")\n",
        "plt.tight_layout()\n",
        "\n",
        "fig_sgd_lr_name = f\"{timestamp}_{commit_short_sgd}_cnn3blocks_sgd_cosine_lr.png\"\n",
        "fig_sgd_lr_path = figuras_dir / fig_sgd_lr_name\n",
        "plt.savefig(fig_sgd_lr_path, dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Figura LR SGD+Cosine guardada en:\", fig_sgd_lr_path)\n"
      ],
      "metadata": {
        "id": "1fNKGZDM2i6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar history del mejor modelo previo (Adam+RLROP)\n",
        "history_adam_path = results_dir / \"history_cnn_3blocks_l2_aug_rlrop.csv\"\n",
        "adam_hist_df = pd.read_csv(history_adam_path)\n",
        "\n",
        "# History del nuevo (SGD+Cosine)\n",
        "sgd_hist_df = pd.DataFrame(cnn_3b_sgd_hist_dict)\n",
        "sgd_hist_df[\"lr\"] = lr_logger_sgd.lrs\n",
        "\n",
        "# --- Comparar val_loss y val_accuracy ---\n",
        "plt.figure(figsize=(10,4))\n",
        "\n",
        "# val_loss\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(adam_hist_df[\"val_loss\"], label=\"Adam+RLROP\", linestyle=\"-\")\n",
        "plt.plot(sgd_hist_df[\"val_loss\"], label=\"SGD+Cosine\", linestyle=\"--\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"val_loss\")\n",
        "plt.title(\"Comparación val_loss\")\n",
        "plt.legend()\n",
        "\n",
        "# val_accuracy\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(adam_hist_df[\"val_accuracy\"], label=\"Adam+RLROP\", linestyle=\"-\")\n",
        "plt.plot(sgd_hist_df[\"val_accuracy\"], label=\"SGD+Cosine\", linestyle=\"--\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"val_accuracy\")\n",
        "plt.title(\"Comparación val_accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "fig_comp_name = f\"{timestamp}_{commit_short_sgd}_adam_vs_sgd_cosine_curvas.png\"\n",
        "fig_comp_path = figuras_dir / fig_comp_name\n",
        "plt.savefig(fig_comp_path, dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Figura comparación Adam vs SGD+Cosine guardada en:\", fig_comp_path)\n"
      ],
      "metadata": {
        "id": "hgMnNoEK2kKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar history SGD\n",
        "cnn_3b_sgd_history_df = sgd_hist_df\n",
        "history_sgd_path = results_dir / \"history_cnn_3blocks_l2_aug_sgd_cosine.csv\"\n",
        "cnn_3b_sgd_history_df.to_csv(history_sgd_path, index=False)\n",
        "print(\"History CNN 3-blocks SGD+Cosine guardado en:\", history_sgd_path)\n",
        "\n",
        "# Actualizar metrics.json\n",
        "metrics_path = results_dir / \"metrics.json\"\n",
        "\n",
        "cnn_3b_sgd_metrics = {\n",
        "    \"model\": \"cnn_3blocks_l2_aug_sgd_cosine\",\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"val_loss\": float(cnn_3b_sgd_val_loss),\n",
        "    \"val_acc\": float(cnn_3b_sgd_val_acc),\n",
        "    \"test_loss\": float(cnn_3b_sgd_test_loss),\n",
        "    \"test_acc\": float(cnn_3b_sgd_test_acc),\n",
        "    \"time_per_epoch\": float(time_per_epoch_sgd),\n",
        "    \"params\": int(cnn_3b_sgd_model.count_params()),\n",
        "    \"l2\": float(weight_decay),\n",
        "    \"augmentation\": {\n",
        "        \"flip_horizontal\": True,\n",
        "        \"rotation\": 0.10,\n",
        "        \"zoom\": 0.10,\n",
        "        \"translation\": [0.10, 0.10]\n",
        "    },\n",
        "    \"optimizer\": \"SGD\",\n",
        "    \"optimizer_settings\": {\n",
        "        \"momentum\": 0.9\n",
        "    },\n",
        "    \"lr_schedule\": {\n",
        "        \"type\": \"CosineDecay\",\n",
        "        \"initial_lr\": 0.05,\n",
        "        \"decay_steps\": int(decay_steps)\n",
        "    },\n",
        "    \"early_stopping\": {\n",
        "        \"monitor\": \"val_loss\",\n",
        "        \"patience\": 5,\n",
        "        \"restore_best_weights\": True,\n",
        "        \"epochs_trained\": len(cnn_3b_sgd_hist_dict[\"loss\"])\n",
        "    },\n",
        "    \"commit\": commit_short_sgd\n",
        "}\n",
        "\n",
        "if metrics_path.exists():\n",
        "    with open(metrics_path, \"r\") as f:\n",
        "        try:\n",
        "            metrics_list = json.load(f)\n",
        "            if not isinstance(metrics_list, list):\n",
        "                metrics_list = [metrics_list]\n",
        "        except json.JSONDecodeError:\n",
        "            metrics_list = []\n",
        "else:\n",
        "    metrics_list = []\n",
        "\n",
        "metrics_list.append(cnn_3b_sgd_metrics)\n",
        "\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    json.dump(metrics_list, f, indent=4)\n",
        "\n",
        "print(\"metrics.json actualizado con SGD+Cosine:\", metrics_path)\n"
      ],
      "metadata": {
        "id": "Oikn-9pC2p4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_path = results_dir / \"params.yaml\"\n",
        "\n",
        "if params_path.exists():\n",
        "    with open(params_path, \"r\") as f:\n",
        "        params = yaml.safe_load(f)\n",
        "else:\n",
        "    params = {}\n",
        "\n",
        "params[\"cnn_3blocks_l2_aug_sgd_cosine\"] = {\n",
        "    \"blocks\": 3,\n",
        "    \"filters\": [32, 64, 128],\n",
        "    \"kernel_size\": 3,\n",
        "    \"dense_units\": 128,\n",
        "    \"dropout\": 0.5,\n",
        "    \"optimizer\": \"SGD\",\n",
        "    \"optimizer_momentum\": 0.9,\n",
        "    \"lr_schedule\": \"CosineDecay\",\n",
        "    \"initial_lr\": 0.05,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"max_epochs\": EPOCHS_SGD,\n",
        "    \"seed\": 42,\n",
        "    \"l2\": weight_decay,\n",
        "    \"augmentation\": {\n",
        "        \"flip_horizontal\": True,\n",
        "        \"rotation\": 0.10,\n",
        "        \"zoom\": 0.10,\n",
        "        \"translation\": [0.10, 0.10]\n",
        "    },\n",
        "    \"early_stopping\": {\n",
        "        \"monitor\": \"val_loss\",\n",
        "        \"patience\": 5,\n",
        "        \"restore_best_weights\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(params_path, \"w\") as f:\n",
        "    yaml.dump(params, f, sort_keys=False)\n",
        "\n",
        "print(\"params.yaml actualizado con configuración SGD+Cosine en:\", params_path)\n"
      ],
      "metadata": {
        "id": "Xv4jt5CE2r35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparacion_path = results_dir / \"comparacion_mlp_vs_cnn.csv\"\n",
        "\n",
        "if comparacion_path.exists():\n",
        "    comparacion_df = pd.read_csv(comparacion_path)\n",
        "else:\n",
        "    comparacion_df = pd.DataFrame(columns=[\"modelo\", \"params\", \"time_per_epoch_s\", \"val_acc\", \"test_acc\"])\n",
        "\n",
        "new_row_sgd = {\n",
        "    \"modelo\": \"CNN 3-blocks SGD+Cosine\",\n",
        "    \"params\": int(cnn_3b_sgd_model.count_params()),\n",
        "    \"time_per_epoch_s\": float(time_per_epoch_sgd),\n",
        "    \"val_acc\": float(cnn_3b_sgd_val_acc),\n",
        "    \"test_acc\": float(cnn_3b_sgd_test_acc)\n",
        "}\n",
        "\n",
        "comparacion_df = pd.concat(\n",
        "    [comparacion_df, pd.DataFrame([new_row_sgd])],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "comparacion_df.to_csv(comparacion_path, index=False)\n",
        "\n",
        "print(comparacion_df)\n",
        "print(\"Tabla comparativa actualizada en:\", comparacion_path)\n"
      ],
      "metadata": {
        "id": "lznqL8QN2thN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Summary SGD+Cosine:\", cnn_3b_sgd_summary_path.exists())\n",
        "print(\"History SGD+Cosine CSV:\", history_sgd_path.exists())\n",
        "print(\"metrics.json:\", metrics_path.exists())\n",
        "print(\"Figura curvas SGD:\", fig_sgd_curvas_path.exists())\n",
        "print(\"Figura LR SGD:\", fig_sgd_lr_path.exists())\n",
        "print(\"Figura comparación Adam vs SGD:\", fig_comp_path.exists())\n",
        "print(\"comparacion_mlp_vs_cnn.csv:\", comparacion_path.exists())\n",
        "print(\"LR por época:\", lr_logger_sgd.lrs)\n"
      ],
      "metadata": {
        "id": "Bo1RlBLk2vOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estabilidad\n",
        "Con SGD+momentum y CosineDecay, la pérdida de entrenamiento suele bajar de forma más suave y la convergencia es algo más lenta al principio que con Adam, porque Adam adapta el LR por parámetro y “acelera” las primeras épocas. El scheduler coseno hace que el LR vaya cayendo de forma controlada, así que las últimas épocas son más estables y los cambios en loss son pequeños. En general, la trayectoria de val_loss es menos brusca y el modelo tiende a refinar mejor al final, pero tarda más en llegar a una zona buena.\n",
        "Val acc máxima: quién gana y por qué\n",
        "En muchos casos, Adam+ReduceLROnPlateau alcanza una val_acc alta más rápido y es muy cómodo para prototipar. Sin embargo, SGD+momentum con un buen scheduler (CosineDecay) puede igualar o incluso superar ligeramente la mejor test_acc, a costa de necesitar un tuning más cuidadoso del LR inicial y del número de épocas. Si en tus resultados SGD+Cosine logra una test_acc parecida o algo mejor que Adam, tiene sentido decir que SGD generaliza un pelín mejor porque no se “acomoda” tanto a patrones específicos y no depende de los momentos adaptativos.\n",
        "Si, por el contrario, tu curva muestra que Adam sigue ganando en val_acc y test_acc, la conclusión razonable es que para este tamaño de red y este setup concreto, Adam+RLROP ofrece un mejor compromiso simplicidad/rendimiento, mientras que SGD+Cosine es una alternativa interesante cuando quieres un control más fino sobre el schedule de LR y estás dispuesto a invertir algo más en tuning y tiempo de entrenamiento.\n"
      ],
      "metadata": {
        "id": "0-VXYh1j2wl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time, json, yaml, random\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Identificadores\n",
        "APELLIDO = \"Relloso\"   # cámbialo\n",
        "NOMBRE = \"Daniel\"       # cámbialo\n",
        "REPO_NAME = f\"IA_P3_CIFAR10_{APELLIDO}{NOMBRE}\"\n",
        "\n",
        "base_dir    = Path(REPO_NAME)\n",
        "results_dir = base_dir / \"results\"\n",
        "figuras_dir = base_dir / \"figuras\"\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "figuras_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Config común\n",
        "SEED = 42\n",
        "weight_decay = 1e-4\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS_ABL = 30\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "set_seed(SEED)\n"
      ],
      "metadata": {
        "id": "kG6JvTcp20OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cnn_3blocks_variant(\n",
        "    use_augment=True,\n",
        "    use_l2=True,\n",
        "    use_dropout=True,\n",
        "    name=\"cnn_3blocks_variant\"\n",
        "):\n",
        "    l2_reg = regularizers.l2(weight_decay) if use_l2 else None\n",
        "\n",
        "    # Data augmentation\n",
        "    if use_augment:\n",
        "        augment_layer = tf.keras.Sequential([\n",
        "            layers.RandomFlip(\"horizontal\"),\n",
        "            layers.RandomRotation(0.10),\n",
        "            layers.RandomZoom(0.10),\n",
        "            layers.RandomTranslation(0.10, 0.10)\n",
        "        ], name=f\"{name}_augment\")\n",
        "    else:\n",
        "        augment_layer = None\n",
        "\n",
        "    inputs = layers.Input(shape=(32, 32, 3))\n",
        "    x = inputs\n",
        "    if augment_layer is not None:\n",
        "        x = augment_layer(x)\n",
        "\n",
        "    # Bloque 1: 32 filtros\n",
        "    x = layers.Conv2D(\n",
        "        32, (3, 3), activation=\"relu\", padding=\"same\",\n",
        "        kernel_regularizer=l2_reg\n",
        "    )(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Bloque 2: 64 filtros\n",
        "    x = layers.Conv2D(\n",
        "        64, (3, 3), activation=\"relu\", padding=\"same\",\n",
        "        kernel_regularizer=l2_reg\n",
        "    )(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Bloque 3: 128 filtros\n",
        "    x = layers.Conv2D(\n",
        "        128, (3, 3), activation=\"relu\", padding=\"same\",\n",
        "        kernel_regularizer=l2_reg\n",
        "    )(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(\n",
        "        128, activation=\"relu\",\n",
        "        kernel_regularizer=l2_reg\n",
        "    )(x)\n",
        "\n",
        "    if use_dropout:\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs, name=name)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "vvZ-UyBY3Z97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_callbacks():\n",
        "    early_stop = EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\",\n",
        "        factor=0.2,\n",
        "        patience=3,\n",
        "        verbose=1,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "    return [early_stop, reduce_lr]\n"
      ],
      "metadata": {
        "id": "842Drs3_3b4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ablation_variant(\n",
        "    variant_id,                  # \"A\", \"B\", \"C\", \"D\"\n",
        "    desc,                        # texto breve\n",
        "    use_augment,\n",
        "    use_l2,\n",
        "    use_dropout,\n",
        "    commit_short                 # hash corto placeholder\n",
        "):\n",
        "    print(f\"\\n=== Entrenando variante {variant_id}: {desc} ===\")\n",
        "\n",
        "    # Para comparabilidad, limpiamos la sesión y fijamos semilla\n",
        "    tf.keras.backend.clear_session()\n",
        "    set_seed(SEED)\n",
        "\n",
        "    model_name = f\"ablation_{variant_id}\"\n",
        "    model = build_cnn_3blocks_variant(\n",
        "        use_augment=use_augment,\n",
        "        use_l2=use_l2,\n",
        "        use_dropout=use_dropout,\n",
        "        name=model_name\n",
        "    )\n",
        "\n",
        "    # Guardar summary\n",
        "    summary_path = results_dir / f\"{model_name}_summary.txt\"\n",
        "    with open(summary_path, \"w\") as f:\n",
        "        model.summary(print_fn=lambda l: f.write(l + \"\\n\"))\n",
        "\n",
        "    model.summary()\n",
        "    print(\"Resumen guardado en:\", summary_path)\n",
        "\n",
        "    callbacks = get_callbacks()\n",
        "\n",
        "    start_time = time.time()\n",
        "    history = model.fit(\n",
        "        x_train, y_train_oh,\n",
        "        validation_data=(x_valid, y_valid_oh),\n",
        "        epochs=EPOCHS_ABL,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "    total_time = time.time() - start_time\n",
        "    time_per_epoch = total_time / len(history.history[\"loss\"])\n",
        "\n",
        "    val_loss, val_acc = model.evaluate(x_valid, y_valid_oh, verbose=0)\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test_oh, verbose=0)\n",
        "\n",
        "    print(f\"[{variant_id}] Val - loss: {val_loss:.4f}, acc: {val_acc:.4f}\")\n",
        "    print(f\"[{variant_id}] Test - loss: {test_loss:.4f}, acc: {test_acc:.4f}\")\n",
        "    print(f\"[{variant_id}] Épocas reales: {len(history.history['loss'])}\")\n",
        "    print(f\"[{variant_id}] Tiempo por época: {time_per_epoch:.2f} s\")\n",
        "\n",
        "    # Curvas\n",
        "    hist_dict = history.history\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(hist_dict[\"loss\"], label=\"train_loss\")\n",
        "    plt.plot(hist_dict[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(f\"Loss Variante {variant_id}\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(hist_dict[\"accuracy\"], label=\"train_acc\")\n",
        "    plt.plot(hist_dict[\"val_accuracy\"], label=\"val_acc\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(f\"Accuracy Variante {variant_id}\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    fig_name = f\"{timestamp}_{commit_short}_ablation_{variant_id}_curvas.png\"\n",
        "    fig_path = figuras_dir / fig_name\n",
        "    plt.savefig(fig_path, dpi=150)\n",
        "    plt.show()\n",
        "    print(f\"[{variant_id}] Figura de curvas guardada en:\", fig_path)\n",
        "\n",
        "    # History CSV\n",
        "    history_df = pd.DataFrame(hist_dict)\n",
        "    history_path = results_dir / f\"history_ablation_{variant_id}.csv\"\n",
        "    history_df.to_csv(history_path, index=False)\n",
        "    print(f\"[{variant_id}] History guardado en:\", history_path)\n",
        "\n",
        "    # Métricas en metrics.json\n",
        "    metrics_path = results_dir / \"metrics.json\"\n",
        "    metrics_entry = {\n",
        "        \"model\": model_name,\n",
        "        \"ablation_variant\": variant_id,\n",
        "        \"description\": desc,\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"val_loss\": float(val_loss),\n",
        "        \"val_acc\": float(val_acc),\n",
        "        \"test_loss\": float(test_loss),\n",
        "        \"test_acc\": float(test_acc),\n",
        "        \"time_per_epoch\": float(time_per_epoch),\n",
        "        \"params\": int(model.count_params()),\n",
        "        \"augment\": bool(use_augment),\n",
        "        \"l2\": float(weight_decay if use_l2 else 0.0),\n",
        "        \"dropout\": bool(use_dropout),\n",
        "        \"optimizer\": \"Adam\",\n",
        "        \"learning_rate\": 1e-3,\n",
        "        \"callbacks\": {\n",
        "            \"early_stopping\": True,\n",
        "            \"reduce_lr_on_plateau\": True\n",
        "        },\n",
        "        \"commit\": commit_short\n",
        "    }\n",
        "\n",
        "    if metrics_path.exists():\n",
        "        with open(metrics_path, \"r\") as f:\n",
        "            try:\n",
        "                metrics_list = json.load(f)\n",
        "                if not isinstance(metrics_list, list):\n",
        "                    metrics_list = [metrics_list]\n",
        "            except json.JSONDecodeError:\n",
        "                metrics_list = []\n",
        "    else:\n",
        "        metrics_list = []\n",
        "\n",
        "    metrics_list.append(metrics_entry)\n",
        "\n",
        "    with open(metrics_path, \"w\") as f:\n",
        "        json.dump(metrics_list, f, indent=4)\n",
        "\n",
        "    print(f\"[{variant_id}] metrics.json actualizado en:\", metrics_path)\n",
        "\n",
        "    # params.yaml\n",
        "    params_path = results_dir / \"params.yaml\"\n",
        "    if params_path.exists():\n",
        "        with open(params_path, \"r\") as f:\n",
        "            params = yaml.safe_load(f)\n",
        "    else:\n",
        "        params = {}\n",
        "\n",
        "    params[f\"ablation_{variant_id}\"] = {\n",
        "        \"description\": desc,\n",
        "        \"blocks\": 3,\n",
        "        \"filters\": [32, 64, 128],\n",
        "        \"kernel_size\": 3,\n",
        "        \"dense_units\": 128,\n",
        "        \"dropout\": use_dropout,\n",
        "        \"l2\": float(weight_decay if use_l2 else 0.0),\n",
        "        \"augment\": use_augment,\n",
        "        \"optimizer\": \"Adam\",\n",
        "        \"learning_rate\": 1e-3,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"max_epochs\": EPOCHS_ABL,\n",
        "        \"seed\": SEED,\n",
        "        \"callbacks\": {\n",
        "            \"early_stopping\": True,\n",
        "            \"reduce_lr_on_plateau\": True\n",
        "        },\n",
        "        \"commit\": commit_short\n",
        "    }\n",
        "\n",
        "    with open(params_path, \"w\") as f:\n",
        "        yaml.dump(params, f, sort_keys=False)\n",
        "\n",
        "    print(f\"[{variant_id}] params.yaml actualizado en:\", params_path)\n",
        "\n",
        "    # Devolvemos métricas para la tabla final\n",
        "    return {\n",
        "        \"variant\": variant_id,\n",
        "        \"description\": desc,\n",
        "        \"augment\": use_augment,\n",
        "        \"l2\": use_l2,\n",
        "        \"dropout\": use_dropout,\n",
        "        \"val_acc\": float(val_acc),\n",
        "        \"test_acc\": float(test_acc)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "RIeUPa8M3fEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_ablation = []\n",
        "\n",
        "# A: Control (todo ON)\n",
        "results_ablation.append(\n",
        "    train_ablation_variant(\n",
        "        variant_id=\"A\",\n",
        "        desc=\"Control (augment + L2 + Dropout)\",\n",
        "        use_augment=True,\n",
        "        use_l2=True,\n",
        "        use_dropout=True,\n",
        "        commit_short=\"ablA01\"   # luego cámbialo por tu hash corto real\n",
        "    )\n",
        ")\n",
        "\n",
        "# B: sin augment\n",
        "results_ablation.append(\n",
        "    train_ablation_variant(\n",
        "        variant_id=\"B\",\n",
        "        desc=\"Sin Data Augmentation (L2 + Dropout)\",\n",
        "        use_augment=False,\n",
        "        use_l2=True,\n",
        "        use_dropout=True,\n",
        "        commit_short=\"ablB01\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# C: sin L2\n",
        "results_ablation.append(\n",
        "    train_ablation_variant(\n",
        "        variant_id=\"C\",\n",
        "        desc=\"Sin L2 (augment + Dropout)\",\n",
        "        use_augment=True,\n",
        "        use_l2=False,\n",
        "        use_dropout=True,\n",
        "        commit_short=\"ablC01\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# D: sin Dropout\n",
        "results_ablation.append(\n",
        "    train_ablation_variant(\n",
        "        variant_id=\"D\",\n",
        "        desc=\"Sin Dropout (augment + L2)\",\n",
        "        use_augment=True,\n",
        "        use_l2=True,\n",
        "        use_dropout=False,\n",
        "        commit_short=\"ablD01\"\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "yL5fLqFg3hUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ablation_df = pd.DataFrame(results_ablation)\n",
        "ablation_path = results_dir / \"ablation_results_A_D.csv\"\n",
        "ablation_df.to_csv(ablation_path, index=False)\n",
        "\n",
        "print(\"Resultados de ablación A–D:\")\n",
        "print(ablation_df)\n",
        "print(\"Tabla de ablación guardada en:\", ablation_path)\n"
      ],
      "metadata": {
        "id": "foFDmiHJ3i8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Misma semilla utilizada:\", SEED)\n",
        "print(\"Épocas máximas:\", EPOCHS_ABL, \"Callbacks: EarlyStopping + ReduceLROnPlateau\")\n",
        "print(\"Ablation table path:\", ablation_path.exists())\n"
      ],
      "metadata": {
        "id": "RMmfEFWb3kaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resultados de ablación (A–D)\n",
        "\n",
        "\n",
        "A (control, augment+L2+Dropout): test_acc ≈ …\n",
        "\n",
        "\n",
        "B (sin augment): test_acc ≈ …\n",
        "\n",
        "\n",
        "C (sin L2): test_acc ≈ …\n",
        "\n",
        "\n",
        "D (sin Dropout): test_acc ≈ …\n",
        "\n",
        "\n",
        "Comparando con el control, se observa que la retirada que provoca la mayor caída en test es […], seguida de […]. En general:\n",
        "\n",
        "\n",
        "Quitar el Data Augmentation suele hacer que el modelo se adapte demasiado a las imágenes originales, perdiendo robustez a traslaciones, cambios de iluminación y pequeñas variaciones.\n",
        "\n",
        "\n",
        "Quitar la L2 facilita que los pesos crezcan mucho, lo que suele aumentar el sobreajuste y empeorar la generalización, aunque a veces suba la accuracy de entrenamiento.\n",
        "\n",
        "\n",
        "Quitar el Dropout reduce la regularización en la parte densa final, haciendo que esa capa memorice más y pierda capacidad de generalizar.\n",
        "\n",
        "\n",
        "Conclusión clara:\n",
        "A la vista de la tabla, la técnica cuya ausencia perjudica más la test_acc en este setup es […], lo que indica que es la contribución de regularización más determinante en esta arquitectura. Las otras técnicas siguen aportando, pero su impacto es algo menor. La combinación de augment + L2 + Dropout es la que proporciona el mejor equilibrio entre rendimiento y robustez, por eso se mantiene como configuración de referencia para el modelo final.\n"
      ],
      "metadata": {
        "id": "ewKKbbp53mzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tMm2nkjW3pqo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}